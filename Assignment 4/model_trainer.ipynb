{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchtext.datasets import TranslationDataset\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import Iterator\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from src.model import TransformerModel\n",
    "from src.helper import save_state, load_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Trainer():\n",
    "    def __init__(self, data_constants : dict, parameters : dict):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available () else \"cpu\")\n",
    "        self.source_field, self.target_field = self._create_fields(batch_first = True)\n",
    "        train_dataset, valid_dataset, self.train_iterator, self.valid_iterator = self._create_dataset(data_constants)\n",
    "        self.source_field.build_vocab(train_dataset)\n",
    "        self.target_field.build_vocab(train_dataset)\n",
    "        self.train_iterator.create_batches\n",
    "        self.valid_iterator.create_batches\n",
    "        self._validation_accuracy = []\n",
    "        self._training_accuracy = []\n",
    "        self._train_loss = []\n",
    "        self._validation_loss = []\n",
    "        self._total_count = []\n",
    "        self.model = TransformerModel(**parameters, src_tokens = len(self.source_field.vocab), tgt_tokens = len(self.target_field.vocab)).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
    "    def greedy_search(self,model,bs,src,seq_length=None):\n",
    "        \"\"\"\n",
    "        We take the source sentence, encode it, and then decode it one word at a time, using the previous\n",
    "        word as the input to the decoder\n",
    "        \n",
    "        :param model: the model to use\n",
    "        :param bs: batch size\n",
    "        :param src: The source sentence\n",
    "        :param seq_length: The length of the sequence to be generated. If None, the sequence will be\n",
    "        generated until the model predicts the end of sentence token\n",
    "        :return: The logits and the argmax of the logits\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        trg = torch.tensor([self.target_field.vocab.stoi[self.target_field.init_token]]*bs).long().to(self.device)\n",
    "        trg = trg.view(bs,-1)\n",
    "        src_key_padding_mask,memory_key_padding_mask,_,src_pos_encoder= model.generate_masks_and_encoding(src,src_embedding=True)\n",
    "        memory_tensor = model.transformer.encoder(src_pos_encoder, src_key_padding_mask = src_key_padding_mask).to(self.device)\n",
    "        count = 1\n",
    "        while True:\n",
    "            tgt_key_padding_mask,tgt_mask,tgt_pos_encoder=model.generate_masks_and_encoding(trg,src_embedding=False)\n",
    "            prediction = model.transformer.decoder(tgt_pos_encoder,memory_tensor, tgt_key_padding_mask = tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask, tgt_mask = tgt_mask)\n",
    "            logits = model.fc(prediction.transpose(1,0))\n",
    "            ix = torch.argmax(logits,dim=2)[:,-1]\n",
    "            trg = torch.cat([trg,ix.view(bs, -1)],dim=1)\n",
    "            if seq_length == None and (ix == self.target_field.vocab.stoi[self.target_field.eos_token]).all():\n",
    "                break\n",
    "            elif seq_length != None and count >= seq_length:\n",
    "                break\n",
    "            count+=1\n",
    "        return logits, torch.argmax(logits,dim=2)\n",
    "\n",
    "    def train_model(self, total_batches_processed, hyperparameter_tuning = False):\n",
    "        \"\"\"\n",
    "        This function takes in a model, optimizer, total number of batches processed, batch size, and a\n",
    "        boolean value for hyperparameter tuning. It then trains the model for 20 epochs, and saves the model\n",
    "        every 500 steps. It also prints out the training and validation accuracy and loss every 500 steps.\n",
    "\n",
    "        :param model: The model that we want to train\n",
    "        :param optimizer: The optimizer used to train the model\n",
    "        :param total_batches_processed: This is the number of batches that have been processed so far. This\n",
    "        is used to save the model after every 500 batches\n",
    "        :param batch_size: The number of examples in each batch\n",
    "        :param hyperparameter_tuning: If you want to tune the hyperparameters, set this to True, defaults to\n",
    "        False (optional)\n",
    "        \"\"\"\n",
    "        loss_fn = nn.CrossEntropyLoss(ignore_index=self.source_field.vocab.stoi[self.source_field.pad_token])\n",
    "        model = self.model.to(self.device)\n",
    "        cost = []\n",
    "        train_accuracy=0\n",
    "        total_train = 0\n",
    "        valid_accuracy = 0\n",
    "        correct_train = 0\n",
    "        for epoch in range(20):\n",
    "            if hyperparameter_tuning == True and valid_accuracy == 100.0: break\n",
    "            print()\n",
    "            print(\"__________________________________________________________________________________________________________________________________________\")\n",
    "            print(f'Epoch Number: {epoch}')\n",
    "            print(\"__________________________________________________________________________________________________________________________________________\")\n",
    "            print()\n",
    "            num_batch_processed = 0\n",
    "            for batch in self.train_iterator:\n",
    "                model.train()\n",
    "                src = batch.src.to(self.device)\n",
    "                trg = batch.trg.to(self.device)\n",
    "                trg_labels = trg[:,1:]\n",
    "                prediction = model(src,trg[:,:-1])\n",
    "                loss = loss_fn(prediction.permute(0,2,1), trg_labels)\n",
    "                cost.append(loss.item())\n",
    "                loss.backward()\n",
    "                num_batch_processed += 1\n",
    "                total_batches_processed += 1\n",
    "                pred_max = torch.argmax(prediction, dim=2)        \n",
    "                total_train += trg_labels.size(0)\n",
    "                correct_train += torch.sum(((pred_max == trg_labels) | (trg_labels == self.source_field.vocab.stoi[self.source_field.pad_token])).all(1))\n",
    "                if num_batch_processed%5==0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "                    self.optimizer.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                if total_batches_processed%500==0:\n",
    "                    train_accuracy = 100* (correct_train/total_train)\n",
    "                    self._training_accuracy.append(train_accuracy)\n",
    "                    total_train = 0\n",
    "                    correct_train = 0\n",
    "                    with torch.no_grad():\n",
    "                        model.eval()\n",
    "                        valid_cost = []\n",
    "                        correct = 0\n",
    "                        total = 0\n",
    "                        for batch in self.valid_iterator:\n",
    "                            src = batch.src.to(self.device)\n",
    "                            trg = batch.trg.to(self.device)\n",
    "                            valid_labels = trg[:,1:]\n",
    "                            prediction, _= self.greedy_search(model, src.size(0),src, valid_labels.size(1))\n",
    "                            loss = loss_fn(prediction.permute(0,2,1),valid_labels)\n",
    "                            valid_cost.append(loss.item())\n",
    "                            total += valid_labels.size(0)\n",
    "                            pred_max = torch.argmax(prediction, dim=2)\n",
    "                            correct += torch.sum(((pred_max == valid_labels) | (valid_labels == self.source_field.vocab.stoi[self.source_field.pad_token])).all(1))\n",
    "                            valid_accuracy = 100 * (correct / total)\n",
    "                            self._validation_accuracy.append(valid_accuracy)\n",
    "                print(\"Steps:\", total_batches_processed)\n",
    "                # sentences = [\"What is the tens digit of 93283843?\", \"What is the units digit of 93215897?\", \"What is the thousands digit of 58179700?\"]\n",
    "                # sentence_targets = [\"4\", \"7\", \"9\"]\n",
    "                # sentences = [\"Put 0.4, 5, 30, 50, -2, 16 in descending order.\",\"Sort -25/127, -2/13, 0.2.\",\"Sort 3, -0.2, 927897, 3/7 in ascending order.\"]\n",
    "                # sentence_targets = [\"50, 30, 16, 5, 0.4, -2\",\"-25/127, -2/13, 0.2\",\"-0.2, 3/7, 3, 927897\"]\n",
    "                sentences = [\"Solve -282*d + 929 - 178 = -1223 for d.\", \"Solve 0 = -i - 91*i - 1598*i - 64220 for i.\", \"Solve -25*m - 2084 = -2559 for m.\"]\n",
    "                sentence_targets = [\"7\", \"-38\", \"19\"]\n",
    "                for sentence in sentences:\n",
    "                    src = self.source_field.process([sentence]).to(self.device)\n",
    "                    _,decoded = self.greedy_search(model, 1, src.view(1,-1))\n",
    "                    pred = [self.target_field.vocab.itos[ind] for ind in decoded[0]]\n",
    "                    print(f\"Example Question: {sentence} | Expected Answer: {sentence_targets[sentences.index(sentence)]} | Generated Answer: {''.join(pred)}\")\n",
    "                self._train_loss.append(np.mean(cost))\n",
    "                self._validation_loss.append(np.mean(valid_cost))\n",
    "                print()\n",
    "                print(f'Train Accuracy {train_accuracy}%')\n",
    "                print(f'Valid Accuracy {valid_accuracy} %')\n",
    "                print(f'Train Loss: {np.mean(cost)}')\n",
    "                print(\"Val Loss:\",np.mean(valid_cost))\n",
    "                print(\"__________________________________________________________________________________________________________________________________________\")\n",
    "                print()\n",
    "                cost = []\n",
    "                self._total_count.append(total_batches_processed)\n",
    "                save_state(\"Results/model.pt\",total_batches_processed)\n",
    "                if hyperparameter_tuning == True and valid_accuracy == 100.0:\n",
    "                    print(\"Final Accuracy for Validation:\", valid_accuracy)\n",
    "                break\n",
    "\n",
    "    def _create_fields(self, batch_first : bool = True):\n",
    "        \"\"\"\n",
    "        It creates two fields, one for the source language and one for the target language\n",
    "        \n",
    "        :param batch_first: If True, the data will be returned in the form of (batch, seq_len, feature),\n",
    "        defaults to True\n",
    "        :type batch_first: bool (optional)\n",
    "        :return: A tuple of two fields, one for the source and one for the target.\n",
    "        \"\"\"\n",
    "        _source_field = Field(tokenize = lambda x: list(x),\n",
    "                            init_token = '<sos>',\n",
    "                            eos_token = '<eos>',\n",
    "                            pad_token = '<pad>',\n",
    "                            batch_first = batch_first)\n",
    "        _target_field = Field(tokenize = lambda x: list(x),\n",
    "                            init_token = '<sos>',\n",
    "                            eos_token = '<eos>',\n",
    "                            pad_token = '<pad>',\n",
    "                            batch_first = batch_first)\n",
    "        return _source_field, _target_field\n",
    "\n",
    "    def _create_dataset(self, data_constants : dict):\n",
    "        \"\"\"\n",
    "        It creates a train and validation dataset from the train and validation files, and then creates an\n",
    "        iterator for each dataset\n",
    "        \n",
    "        :param train_name: the name of the file that contains the training data\n",
    "        :type train_name: str\n",
    "        :param valid_name: the name of the validation file\n",
    "        :type valid_name: str\n",
    "        :param inputs_ending: the ending of the input files, e.g. \".en\"\n",
    "        :type inputs_ending: str\n",
    "        :param targets_ending: the file extension of the target language\n",
    "        :type targets_ending: str\n",
    "        :param self.device: the self.device to run the model on (CPU or GPU)\n",
    "        :type self.device: torch.self.device\n",
    "        \"\"\"\n",
    "        _train_dataset, _valid_dataset, _ = TranslationDataset.splits(\n",
    "            path = data_constants['folder'],\n",
    "            root = data_constants['folder'],\n",
    "            exts = (data_constants['inputs_ending'], data_constants['targets_ending']),\n",
    "            fields = (self.source_field, self.target_field),\n",
    "            train = data_constants['train_name'],\n",
    "            validation = data_constants['valid_name'],\n",
    "            test = data_constants['valid_name']\n",
    "        )\n",
    "        _train_iterator = Iterator(dataset=_train_dataset, batch_size = data_constants['train_batch_size'], train=True, repeat=False, shuffle=True, device=self.device) \n",
    "        _valid_iterator = Iterator(dataset=_valid_dataset, batch_size = data_constants['valid_batch_size'], train=False, repeat=False, shuffle=True, device=self.device) \n",
    "\n",
    "        return _train_dataset, _valid_dataset, _train_iterator, _valid_iterator\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BS = 128\n",
    "VALID_BS = 64\n",
    "DATA_FOLDER = 'data/numbers__place_value'\n",
    "TRAIN_FILE_NAME = \"train\"\n",
    "VALID_FILE_NAME = \"interpolate\"\n",
    "INPUTS_FILE_ENDING = \".x\"\n",
    "TARGETS_FILE_ENDING = \".y\"\n",
    "\n",
    "DIM_MODEL = 512\n",
    "DIM_FEEDFORWARD = 2048\n",
    "NUM_ENCODER_LAYERS = 6\n",
    "NUM_DECODER_LAYERS = 6\n",
    "NUM_HEADS = 8\n",
    "\n",
    "data_constants = {\n",
    "    'folder' : DATA_FOLDER,\n",
    "    'train_name' : TRAIN_FILE_NAME,\n",
    "    'valid_name' : VALID_FILE_NAME,\n",
    "    'inputs_ending' : INPUTS_FILE_ENDING,\n",
    "    'targets_ending' : TARGETS_FILE_ENDING,\n",
    "    'train_batch_size' : TRAIN_BS,\n",
    "    'valid_batch_size' : VALID_BS\n",
    "    }\n",
    "parameters = {\n",
    "    'd_model' : DIM_MODEL,\n",
    "    'nhead' : NUM_HEADS,\n",
    "    'dim_feedforward' : DIM_FEEDFORWARD,\n",
    "    'num_encoder_layers' : NUM_ENCODER_LAYERS,\n",
    "    'num_decoder_layers' : NUM_DECODER_LAYERS\n",
    "}\n",
    "total_batches = 0\n",
    "\n",
    "trainer = Trainer(data_constants=data_constants, parameters=parameters)\n",
    "trainer.train_model(total_batches_processed=total_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(data_constants=data_constants, parameters=parameters)\n",
    "model, optimizer, total_batches, total_count, validation_accuracy, training_accuracy, train_loss, validation_loss = load_state(\"Results/model.pt\",trainer.model,trainer.optimizer, total_batches,trainer._total_count,trainer._validation_accuracy,trainer._training_accuracy,trainer._train_loss,trainer._validation_loss)\n",
    "print(optimizer)\n",
    "state = torch.load(\"drive/MyDrive/model.pt\")\n",
    "print(state[\"validation_acc\"])\n",
    "fig, ax1 = plt.subplots()\n",
    "fig, ax2 = plt.subplots()\n",
    "ax1.plot(total_count,train_loss, 'deepskyblue')\n",
    "ax1.plot(total_count,validation_loss, 'coral')\n",
    "ax1.set_xlabel('Steps')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend([\"Train Loss\", \"Validation Loss\"])\n",
    "ax1.title.set_text(\"Training/Validation Loss for calculus - differentiate\")\n",
    "ax2.plot(total_count,training_accuracy,'deepskyblue')\n",
    "ax2.plot(total_count,validation_accuracy,'coral')\n",
    "ax2.set_xlabel('Steps')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend([\"Train Accuracy\", \"Validation Accuracy\"])\n",
    "ax2.title.set_text(\"Training/Validation Accuracy for calculus - differentiate\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
