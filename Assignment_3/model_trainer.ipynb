{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVXdUxCGGFUo"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3YzLHKS8GFUs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "import numpy as np\n",
        "from torchtext.legacy.data import Field\n",
        "from torchtext.legacy.datasets import LanguageModelingDataset\n",
        "from torchtext.legacy.data import BPTTIterator\n",
        "from src.model import LSTMModel\n",
        "from src.helper import counter, get_fables\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "GKjbsi9UaNCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "EMBEDDING_DIM = 1024\n",
        "HIDDEN_SIZE = 1024\n",
        "LAYERS = 2\n",
        "BPTT_LEN = 256\n",
        "model_parameters = {\n",
        "    'batch_size' : BATCH_SIZE,\n",
        "    'embedding_dim': EMBEDDING_DIM,\n",
        "    'hidden_size': HIDDEN_SIZE,\n",
        "    'num_layers': LAYERS\n",
        "}\n",
        "bonus_path = os.path.join('data', \"donaldtrump.txt\")\n",
        "book_path = os.path.join(os.path.join(\"data\",\"books\"), \"AesopsFables.txt\")"
      ],
      "metadata": {
        "id": "vvY6O70aaMV0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmiBZ3zuGFUu"
      },
      "source": [
        "# Training Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ywD9xE1BGFUu"
      },
      "outputs": [],
      "source": [
        "class Trainer():\n",
        "    def __init__(self, model_parameters : dict, path : str, bptt_len : int, samples : list):\n",
        "        \"\"\"\n",
        "        We create a `LanguageModelingDataset` object from the `path` argument, which is a path to a text\n",
        "        file. We then create a `BPTTIterator` object from the `LanguageModelingDataset` object. We then\n",
        "        create a `LSTMModel` object from the `model_parameters` argument, which is a dictionary of model\n",
        "        parameters. We then create an `Adam` optimizer object from the `LSTMModel` object\n",
        "        \n",
        "        :param model_parameters: a dictionary containing the parameters for the model\n",
        "        :type model_parameters: dict\n",
        "        :param path: the path to the dataset\n",
        "        :type path: str\n",
        "        :param bptt_len: The length of the sequence to be fed into the model\n",
        "        :type bptt_len: int\n",
        "        :param samples: list of strings to be used for training\n",
        "        :type samples: list\n",
        "        \"\"\"\n",
        "        self.samples = samples\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        _split_chars = lambda x: list(x) \n",
        "        self.train_field = Field(tokenize=_split_chars ,init_token ='<sos>',eos_token ='<eos>')\n",
        "        train_dataset = LanguageModelingDataset(\n",
        "            path = path,\n",
        "            text_field=self.train_field\n",
        "        )\n",
        "        self.train_field.build_vocab(train_dataset)\n",
        "        self.bptt_iterator = BPTTIterator(\n",
        "            dataset= train_dataset,\n",
        "            batch_size = model_parameters['batch_size'],\n",
        "            bptt_len = bptt_len,\n",
        "            shuffle = False\n",
        "        )\n",
        "        self.model = LSTMModel(**model_parameters, vocab_size = len(self.train_field.vocab)).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "\n",
        "        \n",
        "    def predict(self, model : LSTMModel, prompt : str,sequence_length : int, method : str = 'greedy') -> str:\n",
        "        \"\"\"\n",
        "        The function takes in a model, a prompt, a sequence length, and a method. It then generates a\n",
        "        sentence of the specified length using the specified method\n",
        "        \n",
        "        :param model: the model to use for prediction\n",
        "        :type model: LSTMModel\n",
        "        :param prompt: The prompt to start the sentence with\n",
        "        :type prompt: str\n",
        "        :param sequence_length: The length of the generated sequence\n",
        "        :type sequence_length: int\n",
        "        :param method: 'greedy' or 'random', defaults to greedy\n",
        "        :type method: str (optional)\n",
        "        :return: A string of the generated sentence\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        generated_sentence=[]\n",
        "        prompt = torch.tensor([self.train_field.vocab.stoi[t] for t in self.train_field.tokenize(prompt)]).long().to(self.device)\n",
        "        hidden = None\n",
        "        Softmax1D = nn.Softmax(dim=1)\n",
        "        if method == 'greedy':\n",
        "            out,hidden=model(prompt.view(-1,1),hidden)\n",
        "            ix = torch.argmax(Softmax1D(out), dim=1)[len(prompt)-1]\n",
        "            for i in range(sequence_length):\n",
        "                out,hidden=model(ix.view(-1,1),hidden)\n",
        "                ix = torch.argmax(Softmax1D(out), dim=1)\n",
        "                generated_sentence.append(self.train_field.vocab.itos[ix])\n",
        "        if method == 'random':\n",
        "            out,hidden=model(prompt.view(-1,1),hidden)\n",
        "            ix = torch.multinomial(Softmax1D(out),1)[len(prompt)-1]\n",
        "            for i in range(sequence_length):\n",
        "                out,hidden=model(ix.view(-1,1),hidden)\n",
        "                ix = torch.multinomial(Softmax1D(out),1)\n",
        "                generated_sentence.append(self.train_field.vocab.itos[ix])\n",
        "        return ''.join(generated_sentence)\n",
        "\n",
        "    def train_model(self, num_epochs : int) -> LSTMModel:\n",
        "        \"\"\"\n",
        "        The function takes in the number of epochs and the model and trains the model for the given\n",
        "        number of epochs\n",
        "        \n",
        "        :param num_epochs: Number of epochs to train for\n",
        "        :return: The model is being returned.\n",
        "        \"\"\"\n",
        "        # vocab_size = \n",
        "        loss_fn = nn.CrossEntropyLoss()\n",
        "        total_steps = 0\n",
        "        loss_plot =[]\n",
        "        perp_plot=[]\n",
        "        for epoch in range(1, num_epochs+1):\n",
        "            cost = 0\n",
        "            num_steps =0\n",
        "            hidden=None\n",
        "            print(\"___________________________________________________________________________\")\n",
        "            print(f\"EPOCH: {epoch}\")\n",
        "            print(f'Total Steps: {total_steps}')\n",
        "            print(\"___________________________________________________________________________\")\n",
        "            for batch in tqdm(self.bptt_iterator):\n",
        "                self.model.train()\n",
        "                self.optimizer.zero_grad()\n",
        "                output, hidden = self.model(batch.text.to(self.device),hidden)\n",
        "                hidden = (hidden[0].detach(), hidden[1].detach())\n",
        "                targets = batch.target\n",
        "                targets = targets.view(targets.shape[0]*targets.shape[1]).to(self.device)\n",
        "                out = output.view(-1, self.model.vocab_size)\n",
        "                loss = loss_fn(out,targets)\n",
        "                cost += loss.item()\n",
        "                num_steps += 1\n",
        "                total_steps+=1\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "            if epoch%10==0:\n",
        "                self.model.eval()\n",
        "                for prompt in self.samples:\n",
        "                    print()\n",
        "                    print('Greedy decoding')\n",
        "                    gen_text = self.predict(self.model, prompt, 100)\n",
        "                    print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "                    print()\n",
        "                    print('Random decoding')\n",
        "                    gen_text = self.predict(self.model, prompt, 100, method='random')\n",
        "                    print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "                    print()\n",
        "\n",
        "            perplexity = np.exp(cost/num_steps)\n",
        "            loss_plot.append(cost/num_steps)\n",
        "            perp_plot.append(perplexity)\n",
        "            print(f\"Training Loss: {cost/num_steps}, \\n Training perplexity: {perplexity}\")\n",
        "        _, ax1 = plt.subplots()\n",
        "        _, ax2 = plt.subplots()\n",
        "        ax1.plot(loss_plot,'coral')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.title.set_text('Train Loss Plot')\n",
        "        ax1.legend([\"Train Loss\"])\n",
        "        ax2.plot(perp_plot,'deepskyblue')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Loss')\n",
        "        ax2.title.set_text('Train Perplexity Plot')\n",
        "        ax2.legend([\"Train Perplexity\"])\n",
        "        plt.show()\n",
        "        return self.model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W20rI4jSGFUx"
      },
      "source": [
        "# Train the fable and the trump models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0Mcc93DPGFUx",
        "outputId": "de9e0086-d751-401b-a4bb-4a785768cc5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on data from: data/books/AesopsFables.txt\n",
            "Fable Analysis\n",
            "Word Count:  5033\n",
            "Line Count:  5033\n",
            "Char Count:  138881\n",
            "Sentence Count:  1880\n",
            "Vocab Count:  108\n",
            "Count:  11\n",
            "___________________________________________________________________________\n",
            "EPOCH: 1\n",
            "Total Steps: 0\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  2.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 3.5015296936035156, \n",
            " Training perplexity: 33.16614721795255\n",
            "___________________________________________________________________________\n",
            "EPOCH: 2\n",
            "Total Steps: 11\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  2.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.7153718038038774, \n",
            " Training perplexity: 15.11022706752168\n",
            "___________________________________________________________________________\n",
            "EPOCH: 3\n",
            "Total Steps: 22\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  2.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.362034494226629, \n",
            " Training perplexity: 10.612520615747519\n",
            "___________________________________________________________________________\n",
            "EPOCH: 4\n",
            "Total Steps: 33\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.1242009943181817, \n",
            " Training perplexity: 8.36621016742036\n",
            "___________________________________________________________________________\n",
            "EPOCH: 5\n",
            "Total Steps: 44\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.9490860267118975, \n",
            " Training perplexity: 7.022266482693182\n",
            "___________________________________________________________________________\n",
            "EPOCH: 6\n",
            "Total Steps: 55\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  2.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.8146450519561768, \n",
            " Training perplexity: 6.1388968012743215\n",
            "___________________________________________________________________________\n",
            "EPOCH: 7\n",
            "Total Steps: 66\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  2.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.7046420899304477, \n",
            " Training perplexity: 5.499417018205378\n",
            "___________________________________________________________________________\n",
            "EPOCH: 8\n",
            "Total Steps: 77\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  2.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.6090871745889836, \n",
            " Training perplexity: 4.998246618281054\n",
            "___________________________________________________________________________\n",
            "EPOCH: 9\n",
            "Total Steps: 88\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  2.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.526662750677629, \n",
            " Training perplexity: 4.602790503100714\n",
            "___________________________________________________________________________\n",
            "EPOCH: 10\n",
            "Total Steps: 99\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Dogs like best to | generated text: the said, “I will be and the Fox and the Fox and the Fox and the Fox and the Fox and the Fox and the\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Dogs like best to | generated text: the bond, and way, has<eos>esten from paw, and lause of hissions<eos>fell eat into<eos>dy often, and atted to be\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: to the said, “I will be and the Fox and the Fox and the Fox and the Fox and the Fox and the Fox and \n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text:  What you hand of had<eos>do, and stighp, the gathing this seerss,” reploated at, he<eos>kill to rempent of \n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A CORTIND AND THE FOX<eos><eos><eos>A WOLF and the Fox and the Fox and the Fox and the Fox and the Fox and the\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>THE MAN AND THE BAT<eos><eos><eos>A GEAMAN was dony daributing out of some our often hemself. He was as then,”\n",
            "\n",
            "Training Loss: 1.4526694254441694, \n",
            " Training perplexity: 4.274509784170795\n",
            "___________________________________________________________________________\n",
            "EPOCH: 11\n",
            "Total Steps: 110\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.389375307343223, \n",
            " Training perplexity: 4.012342788864317\n",
            "___________________________________________________________________________\n",
            "EPOCH: 12\n",
            "Total Steps: 121\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.3393106027082964, \n",
            " Training perplexity: 3.8164115744441025\n",
            "___________________________________________________________________________\n",
            "EPOCH: 13\n",
            "Total Steps: 132\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.2854208729483865, \n",
            " Training perplexity: 3.6161895933633246\n",
            "___________________________________________________________________________\n",
            "EPOCH: 14\n",
            "Total Steps: 143\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.2326143980026245, \n",
            " Training perplexity: 3.4301856943353872\n",
            "___________________________________________________________________________\n",
            "EPOCH: 15\n",
            "Total Steps: 154\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.1828736500306563, \n",
            " Training perplexity: 3.2637395853885867\n",
            "___________________________________________________________________________\n",
            "EPOCH: 16\n",
            "Total Steps: 165\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.1391574577851729, \n",
            " Training perplexity: 3.124135040341952\n",
            "___________________________________________________________________________\n",
            "EPOCH: 17\n",
            "Total Steps: 176\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.097182273864746, \n",
            " Training perplexity: 2.9957130215417895\n",
            "___________________________________________________________________________\n",
            "EPOCH: 18\n",
            "Total Steps: 187\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.0647794550115413, \n",
            " Training perplexity: 2.900199289102725\n",
            "___________________________________________________________________________\n",
            "EPOCH: 19\n",
            "Total Steps: 198\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.0292153033343228, \n",
            " Training perplexity: 2.798868709829268\n",
            "___________________________________________________________________________\n",
            "EPOCH: 20\n",
            "Total Steps: 209\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Dogs like best to | generated text: the water the fables of the food of the food of the food of the food of the food of the food of the \n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Dogs like best to | generated text: get upon its<eos>endeable upon him with of the tirst refund<eos>try the edges, pution the poor Donkeywere ap\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: he was the fables of the food of the food of the food of the food of the food of the food of the foo\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: themed<eos>timeed in<eos>from the primit of a slife, cannot free their happened his xittening courtedicλly.<eos>\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A DOG once came the Fox the same day the fable for the fables for the first before the first that \n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos>The Poor Fables of Ant satiw after her nest that no one keep a mouthful of lionesly deep<eos>big your a\n",
            "\n",
            "Training Loss: 0.9827417839657177, \n",
            " Training perplexity: 2.6717716293541565\n",
            "___________________________________________________________________________\n",
            "EPOCH: 21\n",
            "Total Steps: 220\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.9360931624065746, \n",
            " Training perplexity: 2.5499994982524608\n",
            "___________________________________________________________________________\n",
            "EPOCH: 22\n",
            "Total Steps: 231\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.8929724855856462, \n",
            " Training perplexity: 2.442378807746455\n",
            "___________________________________________________________________________\n",
            "EPOCH: 23\n",
            "Total Steps: 242\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.861543292349035, \n",
            " Training perplexity: 2.3668105572027143\n",
            "___________________________________________________________________________\n",
            "EPOCH: 24\n",
            "Total Steps: 253\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.8406629399819807, \n",
            " Training perplexity: 2.3179030981848285\n",
            "___________________________________________________________________________\n",
            "EPOCH: 25\n",
            "Total Steps: 264\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.7983544523065741, \n",
            " Training perplexity: 2.2218817062890017\n",
            "___________________________________________________________________________\n",
            "EPOCH: 26\n",
            "Total Steps: 275\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.7382796135815707, \n",
            " Training perplexity: 2.092332795424393\n",
            "___________________________________________________________________________\n",
            "EPOCH: 27\n",
            "Total Steps: 286\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.6981368552554738, \n",
            " Training perplexity: 2.0100042877053967\n",
            "___________________________________________________________________________\n",
            "EPOCH: 28\n",
            "Total Steps: 297\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.6558807383884083, \n",
            " Training perplexity: 1.9268388111804573\n",
            "___________________________________________________________________________\n",
            "EPOCH: 29\n",
            "Total Steps: 308\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.612954394383864, \n",
            " Training perplexity: 1.8458767989390834\n",
            "___________________________________________________________________________\n",
            "EPOCH: 30\n",
            "Total Steps: 319\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Dogs like best to | generated text: meet him corracted him to come and straight to the ground,<eos>that he was not a bird, but he was not a \n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Dogs like best to | generated text: Gree yourself as<eos>insubsed on his false made the ambasked of way, she stretched his Mother to strong \n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: that he could not reach the trees, the<eos>statue of a man came to the tree, he mounted to the ground be\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: to her publes for herself<eos>one browsed him by to the one to come out of good turn to his crike laughi\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A MONKEY and a Cat being that he was not a bird, but the man had only took the ground, and the Fox\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A COJUCG, modifify have been thravely traveled that his long throw throwever him that he was not a\n",
            "\n",
            "Training Loss: 0.5988868420774286, \n",
            " Training perplexity: 1.8200916229056532\n",
            "___________________________________________________________________________\n",
            "EPOCH: 31\n",
            "Total Steps: 330\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5768304467201233, \n",
            " Training perplexity: 1.7803864486447958\n",
            "___________________________________________________________________________\n",
            "EPOCH: 32\n",
            "Total Steps: 341\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5365211855281483, \n",
            " Training perplexity: 1.7100475643347197\n",
            "___________________________________________________________________________\n",
            "EPOCH: 33\n",
            "Total Steps: 352\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.4959371333772486, \n",
            " Training perplexity: 1.6420363252955927\n",
            "___________________________________________________________________________\n",
            "EPOCH: 34\n",
            "Total Steps: 363\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.4635348076170141, \n",
            " Training perplexity: 1.5896832898486923\n",
            "___________________________________________________________________________\n",
            "EPOCH: 35\n",
            "Total Steps: 374\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.4412284872748635, \n",
            " Training perplexity: 1.5546158717089067\n",
            "___________________________________________________________________________\n",
            "EPOCH: 36\n",
            "Total Steps: 385\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.4130805324424397, \n",
            " Training perplexity: 1.5114667431413276\n",
            "___________________________________________________________________________\n",
            "EPOCH: 37\n",
            "Total Steps: 396\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.382128352468664, \n",
            " Training perplexity: 1.4654001607909324\n",
            "___________________________________________________________________________\n",
            "EPOCH: 38\n",
            "Total Steps: 407\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.34603663195263257, \n",
            " Training perplexity: 1.4134543924636844\n",
            "___________________________________________________________________________\n",
            "EPOCH: 39\n",
            "Total Steps: 418\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.3213204633105885, \n",
            " Training perplexity: 1.3789474121474437\n",
            "___________________________________________________________________________\n",
            "EPOCH: 40\n",
            "Total Steps: 429\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Dogs like best to | generated text: the danger. Let us be the water.<eos><eos>“What good have to say to the terms of the water, by the way being\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Dogs like best to | generated text: the TartA Bull<eos>do not hart to her by and a Currier stood with it and heard the water.<eos><eos>“Can you swim\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: in the water.<eos><eos>“What good lay, but that was only beak, and a Weasel caught in the tail.”<eos><eos>“No, thank\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: in the bank.<eos><eos><eos><eos><eos>THE APASTING, AND ALD WALFLY MULES<eos><eos><eos>A PLISERAHIDID once made a other onap before t\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A WOMAN had a Hen that no one who had been the dinner of the water, became friends<eos>when he began t\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A WOMAN had a beast as they came to that which the dining room was made a handle to his ress.<eos><eos>He \n",
            "\n",
            "Training Loss: 0.3160240650177002, \n",
            " Training perplexity: 1.3716632643295985\n",
            "___________________________________________________________________________\n",
            "EPOCH: 41\n",
            "Total Steps: 440\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.29525523565032263, \n",
            " Training perplexity: 1.3434692161687665\n",
            "___________________________________________________________________________\n",
            "EPOCH: 42\n",
            "Total Steps: 451\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.2618998478759419, \n",
            " Training perplexity: 1.2993963986030868\n",
            "___________________________________________________________________________\n",
            "EPOCH: 43\n",
            "Total Steps: 462\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.23606735196980563, \n",
            " Training perplexity: 1.266259592372717\n",
            "___________________________________________________________________________\n",
            "EPOCH: 44\n",
            "Total Steps: 473\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.21783268722620877, \n",
            " Training perplexity: 1.2433790170225743\n",
            "___________________________________________________________________________\n",
            "EPOCH: 45\n",
            "Total Steps: 484\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.19976684992963617, \n",
            " Training perplexity: 1.2211180212156714\n",
            "___________________________________________________________________________\n",
            "EPOCH: 46\n",
            "Total Steps: 495\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1782458871603012, \n",
            " Training perplexity: 1.1951191495629214\n",
            "___________________________________________________________________________\n",
            "EPOCH: 47\n",
            "Total Steps: 506\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.15744233741001648, \n",
            " Training perplexity: 1.1705132612094418\n",
            "___________________________________________________________________________\n",
            "EPOCH: 48\n",
            "Total Steps: 517\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.14476064050739462, \n",
            " Training perplexity: 1.1557628943368767\n",
            "___________________________________________________________________________\n",
            "EPOCH: 49\n",
            "Total Steps: 528\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1301800933751193, \n",
            " Training perplexity: 1.1390334972411307\n",
            "___________________________________________________________________________\n",
            "EPOCH: 50\n",
            "Total Steps: 539\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Dogs like best to | generated text: eat at him,<eos>and greedy as to be as desperate as great candbed to be stickeny accepted in the<eos>water. \n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Dogs like best to | generated text: amuse himself at time that was glad at the original Oak. Great returns<eos>or emplieated for this eyes<eos>a\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: in<eos>the collection are in the command<eos>of the river. As the water carried it away remedy about<eos>firm. T\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: in<eos>the Ants was drowned.<eos><eos>The hogs as served a stickane dawned, and knew he away from him to<eos>spring \n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A THRIFTY Woman kept a Hen that could be still to itself of<eos>avainsanguagand to consider what could\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A THRIFTLY to Rode, speeciled the ax that to consider what could be<eos>defense.<eos><eos>Hearing Dog         \n",
            "\n",
            "Training Loss: 0.11900237473574551, \n",
            " Training perplexity: 1.1263725931224366\n",
            "___________________________________________________________________________\n",
            "EPOCH: 51\n",
            "Total Steps: 550\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.11151364784349095, \n",
            " Training perplexity: 1.1179690017673767\n",
            "___________________________________________________________________________\n",
            "EPOCH: 52\n",
            "Total Steps: 561\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.09307985414158214, \n",
            " Training perplexity: 1.0975493756320476\n",
            "___________________________________________________________________________\n",
            "EPOCH: 53\n",
            "Total Steps: 572\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.08056241070682352, \n",
            " Training perplexity: 1.0838964912774982\n",
            "___________________________________________________________________________\n",
            "EPOCH: 54\n",
            "Total Steps: 583\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.06770942156965082, \n",
            " Training perplexity: 1.0700543285917483\n",
            "___________________________________________________________________________\n",
            "EPOCH: 55\n",
            "Total Steps: 594\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.055899836122989655, \n",
            " Training perplexity: 1.0574917559574482\n",
            "___________________________________________________________________________\n",
            "EPOCH: 56\n",
            "Total Steps: 605\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.045805785804986954, \n",
            " Training perplexity: 1.0468710739891962\n",
            "___________________________________________________________________________\n",
            "EPOCH: 57\n",
            "Total Steps: 616\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.040103873068636116, \n",
            " Training perplexity: 1.040918892016541\n",
            "___________________________________________________________________________\n",
            "EPOCH: 58\n",
            "Total Steps: 627\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.03641099431975321, \n",
            " Training perplexity: 1.0370819937214175\n",
            "___________________________________________________________________________\n",
            "EPOCH: 59\n",
            "Total Steps: 638\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.035148761827837334, \n",
            " Training perplexity: 1.0357737809401912\n",
            "___________________________________________________________________________\n",
            "EPOCH: 60\n",
            "Total Steps: 649\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Dogs like best to | generated text: eat at home. And you will get them<eos>easily.”<eos><eos><eos><eos><eos>THE MISER<eos><eos><eos>A MISER who had buried a lump of gold in\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Dogs like best to | generated text: ear about noisela<eos>providing to be acceptaly together.<eos><eos>They were a lot of the food which his friends\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: in perfect security in a<eos>beautiful lake. They were a large company, and were very comfortable,<eos>but t\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: in perfect mean.<eos><eos>The Argodes and the Hen grew very fat and sleek; but she<eos>forgot to elect a Fine wa\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A THRIFTY Woman kept a Hen that could be depended on to lay an egg<eos>every morning. The watchful Wom\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A THRIFTY Woman kept a Hen that could be depended on to lay an egg<eos>every morning. The watchful AND\n",
            "\n",
            "Training Loss: 0.03061210618100383, \n",
            " Training perplexity: 1.0310854746246358\n",
            "___________________________________________________________________________\n",
            "EPOCH: 61\n",
            "Total Steps: 660\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.027635549449107864, \n",
            " Training perplexity: 1.0280209533373255\n",
            "___________________________________________________________________________\n",
            "EPOCH: 62\n",
            "Total Steps: 671\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.023981252685189247, \n",
            " Training perplexity: 1.0242711153775728\n",
            "___________________________________________________________________________\n",
            "EPOCH: 63\n",
            "Total Steps: 682\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.021356963298537514, \n",
            " Training perplexity: 1.0215866555008477\n",
            "___________________________________________________________________________\n",
            "EPOCH: 64\n",
            "Total Steps: 693\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.019896251742135395, \n",
            " Training perplexity: 1.020095501405438\n",
            "___________________________________________________________________________\n",
            "EPOCH: 65\n",
            "Total Steps: 704\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.01767748585817489, \n",
            " Training perplexity: 1.0178346573778334\n",
            "___________________________________________________________________________\n",
            "EPOCH: 66\n",
            "Total Steps: 715\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.016597446562214332, \n",
            " Training perplexity: 1.0167359493817822\n",
            "___________________________________________________________________________\n",
            "EPOCH: 67\n",
            "Total Steps: 726\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.016361320729960094, \n",
            " Training perplexity: 1.016495900101585\n",
            "___________________________________________________________________________\n",
            "EPOCH: 68\n",
            "Total Steps: 737\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.015405807051468979, \n",
            " Training perplexity: 1.0155250882507723\n",
            "___________________________________________________________________________\n",
            "EPOCH: 69\n",
            "Total Steps: 748\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.01482875653627244, \n",
            " Training perplexity: 1.0149392480213617\n",
            "___________________________________________________________________________\n",
            "EPOCH: 70\n",
            "Total Steps: 759\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Dogs like best to | generated text: eat at home, and<eos>he was not a Frog left in the lake. Petrible drinking, let me<eos>ask how you expect to\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Dogs like best to | generated text: eat at home, and he<eos>ground, and a Wolf with his new don’t blow out. Know that<eos>not even the stars nee\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: in perfect security in a<eos>beautiful lake. They were a large company, and were very comfortable,<eos>but t\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: in perfect<eos>station in the world’s literature, and the titles have been<eos>chosen for their genuine appe\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A WOMAN had a Hen that laid an egg every day. The eggs were large, and<eos>sold for a good price. The \n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A MOTHER danger in it it to the other format used in the official Project Gutenberg-tm<eos><eos>Project Gu\n",
            "\n",
            "Training Loss: 0.014682902531190352, \n",
            " Training perplexity: 1.0147912258621976\n",
            "___________________________________________________________________________\n",
            "EPOCH: 71\n",
            "Total Steps: 770\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.01589966370639476, \n",
            " Training perplexity: 1.0160267359346769\n",
            "___________________________________________________________________________\n",
            "EPOCH: 72\n",
            "Total Steps: 781\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0158264593813907, \n",
            " Training perplexity: 1.0159523611055994\n",
            "___________________________________________________________________________\n",
            "EPOCH: 73\n",
            "Total Steps: 792\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.015834820541468533, \n",
            " Training perplexity: 1.0159608556814343\n",
            "___________________________________________________________________________\n",
            "EPOCH: 74\n",
            "Total Steps: 803\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.014655256389894268, \n",
            " Training perplexity: 1.0147631711883849\n",
            "___________________________________________________________________________\n",
            "EPOCH: 75\n",
            "Total Steps: 814\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.013588251122696833, \n",
            " Training perplexity: 1.0136809909884275\n",
            "___________________________________________________________________________\n",
            "EPOCH: 76\n",
            "Total Steps: 825\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.013531879352574999, \n",
            " Training perplexity: 1.013623849607222\n",
            "___________________________________________________________________________\n",
            "EPOCH: 77\n",
            "Total Steps: 836\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.013684585605832663, \n",
            " Training perplexity: 1.0137786481265583\n",
            "___________________________________________________________________________\n",
            "EPOCH: 78\n",
            "Total Steps: 847\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.012998961166224697, \n",
            " Training perplexity: 1.0130838149335784\n",
            "___________________________________________________________________________\n",
            "EPOCH: 79\n",
            "Total Steps: 858\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.012450266121463343, \n",
            " Training perplexity: 1.0125280933391814\n",
            "___________________________________________________________________________\n",
            "EPOCH: 80\n",
            "Total Steps: 869\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Dogs like best to | generated text: eat at home. “Why do you make<eos>see my sight,” said the Boar, “it is time to think of<eos>something else.”\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Dogs like best to | generated text: hearts haym.<eos><eos>On their ard saddle, and the Ant was overjoyed to think he had<eos>beast, and Hound the Ma\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: in the<eos>ground. Another Weasel came out of his hole and caught him.<eos><eos>“Pray don’t kill me,” said the B\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: in the<eos>gra_faces to the hole.<eos><eos>“Look home!” said the most more part of this help proceeded to inaval\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A WOMAN had a Hen that laid an egg every day. The eggs were large, and<eos>sold for a good price. The \n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A WOMAN had a Hen that laid an egg every day. The eggs were large, and<eos>sold for a good price. The \n",
            "\n",
            "Training Loss: 0.012701098807156086, \n",
            " Training perplexity: 1.0127821003354698\n",
            "___________________________________________________________________________\n",
            "EPOCH: 81\n",
            "Total Steps: 880\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.012228052639825777, \n",
            " Training perplexity: 1.01230312094318\n",
            "___________________________________________________________________________\n",
            "EPOCH: 82\n",
            "Total Steps: 891\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.01126992812549526, \n",
            " Training perplexity: 1.0113336730071505\n",
            "___________________________________________________________________________\n",
            "EPOCH: 83\n",
            "Total Steps: 902\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.009960374583236196, \n",
            " Training perplexity: 1.0100101442183116\n",
            "___________________________________________________________________________\n",
            "EPOCH: 84\n",
            "Total Steps: 913\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.00876977624879642, \n",
            " Training perplexity: 1.008808343395832\n",
            "___________________________________________________________________________\n",
            "EPOCH: 85\n",
            "Total Steps: 924\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.008192753080617298, \n",
            " Training perplexity: 1.0082264055214047\n",
            "___________________________________________________________________________\n",
            "EPOCH: 86\n",
            "Total Steps: 935\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.007824167312884872, \n",
            " Training perplexity: 1.0078548560957992\n",
            "___________________________________________________________________________\n",
            "EPOCH: 87\n",
            "Total Steps: 946\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.00798238284716552, \n",
            " Training perplexity: 1.0080143270053885\n",
            "___________________________________________________________________________\n",
            "EPOCH: 88\n",
            "Total Steps: 957\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.007143624169244008, \n",
            " Training perplexity: 1.0071692007191935\n",
            "___________________________________________________________________________\n",
            "EPOCH: 89\n",
            "Total Steps: 968\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.006249055139381777, \n",
            " Training perplexity: 1.0062686212197214\n",
            "___________________________________________________________________________\n",
            "EPOCH: 90\n",
            "Total Steps: 979\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:06<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Dogs like best to | generated text: eat at home. “I have been appointed Nightingale to<eos>these woods, and yet the birds dare laugh at my s\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Dogs like best to | generated text: eat another.<eos><eos><eos><eos><eos>THE BOAR AND THE FOX<eos><eos><eos>A BOAR stood whetting his tusks against a tree.<eos><eos>“What do yo\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: in perfect security in a<eos>beautiful lake. They were a large company, and were very comfortable,<eos>but t\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: in<eos>the obtaining for his life.<eos><eos>In most piteous tones the Mouse said: “Do not eat me. I meant no har\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A WOMAN had a Hen that laid an egg every day. The eggs were large, and<eos>sold for a good price. The \n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A WOMAN had a Hen that laid an egg every day, pending in down to the little<eos>stiff on his sheep for\n",
            "\n",
            "Training Loss: 0.006057176569646055, \n",
            " Training perplexity: 1.0060755583588168\n",
            "___________________________________________________________________________\n",
            "EPOCH: 91\n",
            "Total Steps: 990\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.006069232921370051, \n",
            " Training perplexity: 1.006087688032729\n",
            "___________________________________________________________________________\n",
            "EPOCH: 92\n",
            "Total Steps: 1001\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.006697755222293464, \n",
            " Training perplexity: 1.0067202353455649\n",
            "___________________________________________________________________________\n",
            "EPOCH: 93\n",
            "Total Steps: 1012\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.006226048889485272, \n",
            " Training perplexity: 1.0062454710186592\n",
            "___________________________________________________________________________\n",
            "EPOCH: 94\n",
            "Total Steps: 1023\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.006246887570755048, \n",
            " Training perplexity: 1.0062664400657917\n",
            "___________________________________________________________________________\n",
            "EPOCH: 95\n",
            "Total Steps: 1034\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.005956578407097946, \n",
            " Training perplexity: 1.0059743540968282\n",
            "___________________________________________________________________________\n",
            "EPOCH: 96\n",
            "Total Steps: 1045\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.005873646057972854, \n",
            " Training perplexity: 1.00589092973982\n",
            "___________________________________________________________________________\n",
            "EPOCH: 97\n",
            "Total Steps: 1056\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.005958105141127651, \n",
            " Training perplexity: 1.00597588995328\n",
            "___________________________________________________________________________\n",
            "EPOCH: 98\n",
            "Total Steps: 1067\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.005229503297331658, \n",
            " Training perplexity: 1.0052432010167138\n",
            "___________________________________________________________________________\n",
            "EPOCH: 99\n",
            "Total Steps: 1078\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0051056962587277994, \n",
            " Training perplexity: 1.005118752536877\n",
            "___________________________________________________________________________\n",
            "EPOCH: 100\n",
            "Total Steps: 1089\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:05<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Dogs like best to | generated text: eat at home. “I have found nothing to be wary my brook<eos>and was my could only lick the brim for the b\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Dogs like best to | generated text: eat out.<eos><eos>“My dear friend,” said the Wolf, “be careful! I am afraid you will fall<eos>and break your not\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: in perfect security in a<eos>beautiful lake. They were a large company, and were very comfortable,<eos>but t\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THERE were once some Frogs who lived together | generated text: in perfect security in a<eos>beautiful lake. They were a large company, and were very comfortable,<eos>but t\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A WOMAN had a Hen that laid an egg every day. The eggs were large, and<eos>sold for a good price. The \n",
            "\n",
            "Random decoding\n",
            "Sample prompt: THE WOMAN AND HER HEN | generated text: <eos><eos>A WOMAN had a Hen that laid an egg every day. The eggs were large, and<eos>sold for a good price. The \n",
            "\n",
            "Training Loss: 0.00526842593469403, \n",
            " Training perplexity: 1.0052823284947552\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "fable_samples = ['Dogs like best to', 'THERE were once some Frogs who lived together', 'THE WOMAN AND HER HEN']\n",
        "get_fables()\n",
        "PATH = book_path\n",
        "trainer = Trainer(model_parameters = model_parameters, path = PATH, bptt_len = BPTT_LEN, samples = fable_samples)\n",
        "print(f\"Training on data from: {PATH}\")\n",
        "print(\"Fable Analysis\")\n",
        "counter(PATH)\n",
        "print(\"Vocab Count: \", len(trainer.train_field.vocab))\n",
        "print(\"Count: \", len(trainer.bptt_iterator))\n",
        "fable_model = trainer.train_model(num_epochs=100)\n",
        "torch.save(fable_model.state_dict(), 'fable_model.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pSfdOw1FGFUy",
        "outputId": "4172a8c4-2515-4527-e0d9-7a3006fe2990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Count:  1785\n",
            "Line Count:  1785\n",
            "Char Count:  212573\n",
            "Sentence Count:  4852\n",
            "Vocab Count:  86\n",
            "Count:  16\n",
            "___________________________________________________________________________\n",
            "EPOCH: 1\n",
            "Total Steps: 0\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 3.2571451514959335, \n",
            " Training perplexity: 25.975275713377215\n",
            "___________________________________________________________________________\n",
            "EPOCH: 2\n",
            "Total Steps: 16\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 2.4017930030822754, \n",
            " Training perplexity: 11.042958699448073\n",
            "___________________________________________________________________________\n",
            "EPOCH: 3\n",
            "Total Steps: 32\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.9851548224687576, \n",
            " Training perplexity: 7.280174432783706\n",
            "___________________________________________________________________________\n",
            "EPOCH: 4\n",
            "Total Steps: 48\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.7022864744067192, \n",
            " Training perplexity: 5.486477752057189\n",
            "___________________________________________________________________________\n",
            "EPOCH: 5\n",
            "Total Steps: 64\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.5073883086442947, \n",
            " Training perplexity: 4.514923795458787\n",
            "___________________________________________________________________________\n",
            "EPOCH: 6\n",
            "Total Steps: 80\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.3672349900007248, \n",
            " Training perplexity: 3.9244844410209088\n",
            "___________________________________________________________________________\n",
            "EPOCH: 7\n",
            "Total Steps: 96\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.2644379809498787, \n",
            " Training perplexity: 3.541102009960352\n",
            "___________________________________________________________________________\n",
            "EPOCH: 8\n",
            "Total Steps: 112\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.1861360594630241, \n",
            " Training perplexity: 3.2744046276046297\n",
            "___________________________________________________________________________\n",
            "EPOCH: 9\n",
            "Total Steps: 128\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.1239720061421394, \n",
            " Training perplexity: 3.0770520319013994\n",
            "___________________________________________________________________________\n",
            "EPOCH: 10\n",
            "Total Steps: 144\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Good morning America | generated text:  people that we will be the state of the problem the world and the world and the way we will be the \n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Good morning America | generated text:  Adread Washington Michiga walk about this thing, nuclup expers in the Bacmoss, and we reasons, you \n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Very good | generated text: to the problem the world and the world and the way we will be the state of the problem the world and\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Very good | generated text:  We whoweven to come. Thill he swinped her want to we cut beor. We’ve been to do, unforces I could f\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:01:00)<eos>I said, “What we will be the state of the problem the world and the world and the way we \n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Donald Trump: | generated text: (10:59)<eos>Now it’s very nimage First taxkes of door as bectar.<eos><eos>Vice President Donald J. Trump: (14:23\n",
            "\n",
            "Training Loss: 1.0694655776023865, \n",
            " Training perplexity: 2.9138218721273432\n",
            "___________________________________________________________________________\n",
            "EPOCH: 11\n",
            "Total Steps: 160\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 1.021485198289156, \n",
            " Training perplexity: 2.777316568173853\n",
            "___________________________________________________________________________\n",
            "EPOCH: 12\n",
            "Total Steps: 176\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.9770585708320141, \n",
            " Training perplexity: 2.656630447774579\n",
            "___________________________________________________________________________\n",
            "EPOCH: 13\n",
            "Total Steps: 192\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.9334794841706753, \n",
            " Training perplexity: 2.5433433224025026\n",
            "___________________________________________________________________________\n",
            "EPOCH: 14\n",
            "Total Steps: 208\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.8985811546444893, \n",
            " Training perplexity: 2.456115789276545\n",
            "___________________________________________________________________________\n",
            "EPOCH: 15\n",
            "Total Steps: 224\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.8648763820528984, \n",
            " Training perplexity: 2.374712510746367\n",
            "___________________________________________________________________________\n",
            "EPOCH: 16\n",
            "Total Steps: 240\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.8286845274269581, \n",
            " Training perplexity: 2.2903039257477804\n",
            "___________________________________________________________________________\n",
            "EPOCH: 17\n",
            "Total Steps: 256\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.7999284192919731, \n",
            " Training perplexity: 2.225381628398538\n",
            "___________________________________________________________________________\n",
            "EPOCH: 18\n",
            "Total Steps: 272\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.7654283158481121, \n",
            " Training perplexity: 2.1499150201528425\n",
            "___________________________________________________________________________\n",
            "EPOCH: 19\n",
            "Total Steps: 288\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.7253978252410889, \n",
            " Training perplexity: 2.065552665523037\n",
            "___________________________________________________________________________\n",
            "EPOCH: 20\n",
            "Total Steps: 304\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Good morning America | generated text:  flag. And we will end our reliance on China on the state of Michigan, we will make America great ag\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Good morning America | generated text: s flast, counting for that? I saw so you from $10 million joins to continue to come the smart on Chi\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Very good | generated text:  And we will end our reliance on China on the state of Michigan, we will make America great again. T\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Very good | generated text:  And I think you’w ever wene. She reject on anyway, which was one people on really well. The great j\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:01:00)<eos>Well, he said, “I had a lot of work to do what we have to do what we have to do what we h\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Donald Trump: | generated text: (30:43)<eos>One, many is that he spent the disease. It wouldn’t feel that none so much. I said, “Becanso\n",
            "\n",
            "Training Loss: 0.6990790218114853, \n",
            " Training perplexity: 2.011898938921065\n",
            "___________________________________________________________________________\n",
            "EPOCH: 21\n",
            "Total Steps: 320\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.6751954779028893, \n",
            " Training perplexity: 1.964416938544072\n",
            "___________________________________________________________________________\n",
            "EPOCH: 22\n",
            "Total Steps: 336\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.6335835196077824, \n",
            " Training perplexity: 1.8843511037785325\n",
            "___________________________________________________________________________\n",
            "EPOCH: 23\n",
            "Total Steps: 352\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5961959660053253, \n",
            " Training perplexity: 1.8152005654745147\n",
            "___________________________________________________________________________\n",
            "EPOCH: 24\n",
            "Total Steps: 368\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5675549805164337, \n",
            " Training perplexity: 1.7639488851923377\n",
            "___________________________________________________________________________\n",
            "EPOCH: 25\n",
            "Total Steps: 384\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5451462306082249, \n",
            " Training perplexity: 1.7248605913490263\n",
            "___________________________________________________________________________\n",
            "EPOCH: 26\n",
            "Total Steps: 400\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5141462776809931, \n",
            " Training perplexity: 1.6722102889899157\n",
            "___________________________________________________________________________\n",
            "EPOCH: 27\n",
            "Total Steps: 416\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.48824636824429035, \n",
            " Training perplexity: 1.6294562467995002\n",
            "___________________________________________________________________________\n",
            "EPOCH: 28\n",
            "Total Steps: 432\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.462318466976285, \n",
            " Training perplexity: 1.5877508689366553\n",
            "___________________________________________________________________________\n",
            "EPOCH: 29\n",
            "Total Steps: 448\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.42731332406401634, \n",
            " Training perplexity: 1.5331329539243383\n",
            "___________________________________________________________________________\n",
            "EPOCH: 30\n",
            "Total Steps: 464\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Good morning America | generated text:  dream. And as I said, “What the hell is going on? Is that the president said, “No, no, no, no, not \n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Good morning America | generated text:  dream. That was a Trump back, and I appreciate it. That’s all I case, but we want up, and we nave i\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Very good | generated text: ratings that they have to come in the one that nobody believed. It was one of the most invest the oi\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Very good | generated text: ratings to many of you have been to call Ever.<eos><eos>Donald Trump: (01:58:07)<eos>How a by the way, and that’\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:01:03)<eos>… some great warriors. I don’t know. I don’t know. I don’t know. I don’t know. I don’t kn\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:27:19)<eos>The only Marson and Kamala Harris is not good for the United States. I could be great for\n",
            "\n",
            "Training Loss: 0.4022822491824627, \n",
            " Training perplexity: 1.495233301500003\n",
            "___________________________________________________________________________\n",
            "EPOCH: 31\n",
            "Total Steps: 480\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.37967382557690144, \n",
            " Training perplexity: 1.4618077073794524\n",
            "___________________________________________________________________________\n",
            "EPOCH: 32\n",
            "Total Steps: 496\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.34847475588321686, \n",
            " Training perplexity: 1.4169047739623728\n",
            "___________________________________________________________________________\n",
            "EPOCH: 33\n",
            "Total Steps: 512\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.3280080817639828, \n",
            " Training perplexity: 1.3882001913503847\n",
            "___________________________________________________________________________\n",
            "EPOCH: 34\n",
            "Total Steps: 528\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.30459695030003786, \n",
            " Training perplexity: 1.3560783258822273\n",
            "___________________________________________________________________________\n",
            "EPOCH: 35\n",
            "Total Steps: 544\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.27801018580794334, \n",
            " Training perplexity: 1.3204996474963928\n",
            "___________________________________________________________________________\n",
            "EPOCH: 36\n",
            "Total Steps: 560\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.2531714681535959, \n",
            " Training perplexity: 1.2881041267359776\n",
            "___________________________________________________________________________\n",
            "EPOCH: 37\n",
            "Total Steps: 576\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.2338552512228489, \n",
            " Training perplexity: 1.2634615944502097\n",
            "___________________________________________________________________________\n",
            "EPOCH: 38\n",
            "Total Steps: 592\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.2146493373438716, \n",
            " Training perplexity: 1.239427199920504\n",
            "___________________________________________________________________________\n",
            "EPOCH: 39\n",
            "Total Steps: 608\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.19454912468791008, \n",
            " Training perplexity: 1.21476315628025\n",
            "___________________________________________________________________________\n",
            "EPOCH: 40\n",
            "Total Steps: 624\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Good morning America | generated text:  blood and treasure in endless foreign, ridiculous working all the time. Thank you very much, I appr\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Good morning America | generated text:  got each and sent and women of Willient New York 91p, some beautiful, beautiful Abriotits, ran expl\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Very good | generated text: ratings the things that the hell out of our country. I see who’s had so many different problem. Ever\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Very good | generated text: ratings the things that the great Whith you ever seen. And over administration?<eos><eos>Donald Trump: (02:3\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:21:11)<eos>No, we’ve done a good job. They loved how to do it. It’s not fair. You’re sending them ba\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:51:41)<eos>Joe Biden stated with oil against fracking. The only bad under President and Elicuments. \n",
            "\n",
            "Training Loss: 0.1802372820675373, \n",
            " Training perplexity: 1.1975014750390218\n",
            "___________________________________________________________________________\n",
            "EPOCH: 41\n",
            "Total Steps: 640\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.17467504926025867, \n",
            " Training perplexity: 1.190859183159865\n",
            "___________________________________________________________________________\n",
            "EPOCH: 42\n",
            "Total Steps: 656\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.162528021261096, \n",
            " Training perplexity: 1.1764812844760604\n",
            "___________________________________________________________________________\n",
            "EPOCH: 43\n",
            "Total Steps: 672\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.14813608769327402, \n",
            " Training perplexity: 1.159670702535857\n",
            "___________________________________________________________________________\n",
            "EPOCH: 44\n",
            "Total Steps: 688\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1386572546325624, \n",
            " Training perplexity: 1.148730310530332\n",
            "___________________________________________________________________________\n",
            "EPOCH: 45\n",
            "Total Steps: 704\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.1266387738287449, \n",
            " Training perplexity: 1.1350069495075306\n",
            "___________________________________________________________________________\n",
            "EPOCH: 46\n",
            "Total Steps: 720\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.11229809606447816, \n",
            " Training perplexity: 1.1188463346280433\n",
            "___________________________________________________________________________\n",
            "EPOCH: 47\n",
            "Total Steps: 736\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.09663460543379188, \n",
            " Training perplexity: 1.1014578333752574\n",
            "___________________________________________________________________________\n",
            "EPOCH: 48\n",
            "Total Steps: 752\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.08705348987132311, \n",
            " Training perplexity: 1.0909550332019489\n",
            "___________________________________________________________________________\n",
            "EPOCH: 49\n",
            "Total Steps: 768\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.07698982208967209, \n",
            " Training perplexity: 1.080031083877137\n",
            "___________________________________________________________________________\n",
            "EPOCH: 50\n",
            "Total Steps: 784\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Good morning America | generated text: strong again. We have made America proud again. We have made America safe again, and we will make Am\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Good morning America | generated text:  economy from the world, and now they all was the great plague, and they want to touch. They want to\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Very good | generated text: ratings the other night, a fake report, but the ratings go through the roof. 60 Minutes had very goo\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Very good | generated text: allying the fastest growth, by the way, do. I think they’d been fighting for 400 years, you want to \n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:11:03)<eos>For years, you had a president who apologized for America. Now you have a president who i\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:01:24)<eos>A vote for sleepy Joe, at the announcer, ran refugees and what’s right for the White Hous\n",
            "\n",
            "Training Loss: 0.07014856371097267, \n",
            " Training perplexity: 1.0726675288860168\n",
            "___________________________________________________________________________\n",
            "EPOCH: 51\n",
            "Total Steps: 800\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.06545031210407615, \n",
            " Training perplexity: 1.0676396872258722\n",
            "___________________________________________________________________________\n",
            "EPOCH: 52\n",
            "Total Steps: 816\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.05995371378958225, \n",
            " Training perplexity: 1.0617873992929658\n",
            "___________________________________________________________________________\n",
            "EPOCH: 53\n",
            "Total Steps: 832\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.050775231095030904, \n",
            " Training perplexity: 1.0520863903990496\n",
            "___________________________________________________________________________\n",
            "EPOCH: 54\n",
            "Total Steps: 848\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0448023471981287, \n",
            " Training perplexity: 1.0458211300024163\n",
            "___________________________________________________________________________\n",
            "EPOCH: 55\n",
            "Total Steps: 864\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.03998419945128262, \n",
            " Training perplexity: 1.0407943289409674\n",
            "___________________________________________________________________________\n",
            "EPOCH: 56\n",
            "Total Steps: 880\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.036542543675750494, \n",
            " Training perplexity: 1.0372184301636742\n",
            "___________________________________________________________________________\n",
            "EPOCH: 57\n",
            "Total Steps: 896\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.03209973324555904, \n",
            " Training perplexity: 1.032620486762271\n",
            "___________________________________________________________________________\n",
            "EPOCH: 58\n",
            "Total Steps: 912\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.027843429939821362, \n",
            " Training perplexity: 1.0282346810517102\n",
            "___________________________________________________________________________\n",
            "EPOCH: 59\n",
            "Total Steps: 928\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.023934513214044273, \n",
            " Training perplexity: 1.0242232426061137\n",
            "___________________________________________________________________________\n",
            "EPOCH: 60\n",
            "Total Steps: 944\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Good morning America | generated text:  blood and treasure in endless foreign wars in countries that most of you have never even heard of, \n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Good morning America | generated text:  unemployment all reached their lowest levels, ever recorded. Think of that, what we had going and w\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Very good | generated text:  She would call me, I’d come. I’d make a speech, I’d leave. Call me the next day.<eos><eos>Donald Trump: (01\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Very good | generated text:  State, You know, we have to wrong for a long time. You’re going to do the job we’ve done, when we h\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:11:03)<eos>Michigan gave us Motown, the Mustang and the unrivaled might of the American Midwest, the\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:01:09)<eos>“Sir, you have to do this? You have to get out the great people of Michigan.<eos><eos>Donald Trum\n",
            "\n",
            "Training Loss: 0.022392700775526464, \n",
            " Training perplexity: 1.0226452992297956\n",
            "___________________________________________________________________________\n",
            "EPOCH: 61\n",
            "Total Steps: 960\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.020796125754714012, \n",
            " Training perplexity: 1.0210138719844375\n",
            "___________________________________________________________________________\n",
            "EPOCH: 62\n",
            "Total Steps: 976\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.01960402785334736, \n",
            " Training perplexity: 1.019797448682266\n",
            "___________________________________________________________________________\n",
            "EPOCH: 63\n",
            "Total Steps: 992\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.018446171190589666, \n",
            " Training perplexity: 1.0186173527344635\n",
            "___________________________________________________________________________\n",
            "EPOCH: 64\n",
            "Total Steps: 1008\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.017899772385135293, \n",
            " Training perplexity: 1.018060933456972\n",
            "___________________________________________________________________________\n",
            "EPOCH: 65\n",
            "Total Steps: 1024\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.016348902776371688, \n",
            " Training perplexity: 1.016483277381049\n",
            "___________________________________________________________________________\n",
            "EPOCH: 66\n",
            "Total Steps: 1040\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.015064496721606702, \n",
            " Training perplexity: 1.0151785381918397\n",
            "___________________________________________________________________________\n",
            "EPOCH: 67\n",
            "Total Steps: 1056\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.012971938063856214, \n",
            " Training perplexity: 1.0130564386358374\n",
            "___________________________________________________________________________\n",
            "EPOCH: 68\n",
            "Total Steps: 1072\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.011608745087869465, \n",
            " Training perplexity: 1.0116763880657444\n",
            "___________________________________________________________________________\n",
            "EPOCH: 69\n",
            "Total Steps: 1088\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.010624007845763117, \n",
            " Training perplexity: 1.0106806430035449\n",
            "___________________________________________________________________________\n",
            "EPOCH: 70\n",
            "Total Steps: 1104\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Good morning America | generated text:  blood and treasure in endless foreign, ridiculous wars. Countries you’ve never even heard of. In 20\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Good morning America | generated text:  blood and treasure on foreign Wars. You know what they call them? The endless foreign wars, countri\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Very good | generated text: all over. We have to do it. It’s just we could be a little bit superstitious, right? But you people,\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Very good | generated text: all over the last few years. A lot of great things. And you’re paying, what, $2 a gallon for your ga\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:51:31)<eos>No, he’s the best loser I’ve ever seen. He could lose … He lost with Hillary and they did\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:52:41)<eos>o let me saved to the people of Michigan. Now it’s our taxes.” I never heard that before.\n",
            "\n",
            "Training Loss: 0.009756486106198281, \n",
            " Training perplexity: 1.0098042357801063\n",
            "___________________________________________________________________________\n",
            "EPOCH: 71\n",
            "Total Steps: 1120\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.008756377879763022, \n",
            " Training perplexity: 1.0087948270999114\n",
            "___________________________________________________________________________\n",
            "EPOCH: 72\n",
            "Total Steps: 1136\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.007955691777169704, \n",
            " Training perplexity: 1.007987422383488\n",
            "___________________________________________________________________________\n",
            "EPOCH: 73\n",
            "Total Steps: 1152\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.007351701467996463, \n",
            " Training perplexity: 1.0073787915706587\n",
            "___________________________________________________________________________\n",
            "EPOCH: 74\n",
            "Total Steps: 1168\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0067021446011494845, \n",
            " Training perplexity: 1.0067246542317778\n",
            "___________________________________________________________________________\n",
            "EPOCH: 75\n",
            "Total Steps: 1184\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.006214951368747279, \n",
            " Training perplexity: 1.0062343042506388\n",
            "___________________________________________________________________________\n",
            "EPOCH: 76\n",
            "Total Steps: 1200\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0059674931690096855, \n",
            " Training perplexity: 1.0059853341273146\n",
            "___________________________________________________________________________\n",
            "EPOCH: 77\n",
            "Total Steps: 1216\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.005637408728944138, \n",
            " Training perplexity: 1.0056533288194924\n",
            "___________________________________________________________________________\n",
            "EPOCH: 78\n",
            "Total Steps: 1232\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.00524668357684277, \n",
            " Training perplexity: 1.0052604715242395\n",
            "___________________________________________________________________________\n",
            "EPOCH: 79\n",
            "Total Steps: 1248\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.004952140967361629, \n",
            " Training perplexity: 1.004964423083329\n",
            "___________________________________________________________________________\n",
            "EPOCH: 80\n",
            "Total Steps: 1264\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Good morning America | generated text:  blood and treasure in endless foreign, ridiculous wars. Countries you’ve never even heard of. In 20\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Good morning America | generated text:  plague and we’re going to pull it tomorrow. So I got to ask you to do a couple of things. Air Force\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Very good | generated text: ratings the other night, a fake report, but the ratings go through the roof.<eos><eos>Donald Trump: (11:35)<eos>\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Very good | generated text: ratings the other night, a fake report, but the ratings go through the roof.<eos><eos>Donald Trump: (11:35)<eos>\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:40:53)<eos>This is the only man that I’ve ever seen run for office on the fact that he wants to give\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:41:03)<eos>You know what? I wasn’t going to take my chance. I said, “You have something, give it to \n",
            "\n",
            "Training Loss: 0.004761239979416132, \n",
            " Training perplexity: 1.0047725926930005\n",
            "___________________________________________________________________________\n",
            "EPOCH: 81\n",
            "Total Steps: 1280\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.004579582076985389, \n",
            " Training perplexity: 1.0045900843889317\n",
            "___________________________________________________________________________\n",
            "EPOCH: 82\n",
            "Total Steps: 1296\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0044949140574317425, \n",
            " Training perplexity: 1.0045050313367108\n",
            "___________________________________________________________________________\n",
            "EPOCH: 83\n",
            "Total Steps: 1312\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.004183710261713713, \n",
            " Training perplexity: 1.0041924741951478\n",
            "___________________________________________________________________________\n",
            "EPOCH: 84\n",
            "Total Steps: 1328\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.003944873664295301, \n",
            " Training perplexity: 1.0039526649202135\n",
            "___________________________________________________________________________\n",
            "EPOCH: 85\n",
            "Total Steps: 1344\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.003789000169490464, \n",
            " Training perplexity: 1.0037961875053714\n",
            "___________________________________________________________________________\n",
            "EPOCH: 86\n",
            "Total Steps: 1360\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.00366502815450076, \n",
            " Training perplexity: 1.0036717525827508\n",
            "___________________________________________________________________________\n",
            "EPOCH: 87\n",
            "Total Steps: 1376\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0036175523709971458, \n",
            " Training perplexity: 1.0036241036110114\n",
            "___________________________________________________________________________\n",
            "EPOCH: 88\n",
            "Total Steps: 1392\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.00347554667678196, \n",
            " Training perplexity: 1.0034815933923178\n",
            "___________________________________________________________________________\n",
            "EPOCH: 89\n",
            "Total Steps: 1408\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.003400022746063769, \n",
            " Training perplexity: 1.0034058093797709\n",
            "___________________________________________________________________________\n",
            "EPOCH: 90\n",
            "Total Steps: 1424\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Good morning America | generated text:  plague. I’m make a speech, I’d leave. Call me the next day.<eos><eos>Donald Trump: (01:01:09)<eos>“Sir, you hav\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Good morning America | generated text:  blood and treasure on foreign Wars. You know what they call them? The endless foreign wars, countri\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Very good | generated text: ratings the other night, a fake report, but the ratings go through the roof.<eos><eos>Donald Trump: (11:35)<eos>\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Very good | generated text:  She did a great job. Thank you, honey, thank you very much. And Michael, thank you very much. Thank\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:40:50)<eos>This group over here. They’re an activist group. Thank you very much. Thank you, fellas.<eos>\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:09:24)<eos>America will land the first woman on the moon, and the United States will be the first na\n",
            "\n",
            "Training Loss: 0.0032235033431788906, \n",
            " Training perplexity: 1.0032287044171389\n",
            "___________________________________________________________________________\n",
            "EPOCH: 91\n",
            "Total Steps: 1440\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0032626007014187053, \n",
            " Training perplexity: 1.0032679287759714\n",
            "___________________________________________________________________________\n",
            "EPOCH: 92\n",
            "Total Steps: 1456\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0032148269965546206, \n",
            " Training perplexity: 1.0032200000949167\n",
            "___________________________________________________________________________\n",
            "EPOCH: 93\n",
            "Total Steps: 1472\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.003133122925646603, \n",
            " Training perplexity: 1.0031380362853268\n",
            "___________________________________________________________________________\n",
            "EPOCH: 94\n",
            "Total Steps: 1488\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.003094815998338163, \n",
            " Training perplexity: 1.0030996098854938\n",
            "___________________________________________________________________________\n",
            "EPOCH: 95\n",
            "Total Steps: 1504\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.00312476851104293, \n",
            " Training perplexity: 1.0031296556892744\n",
            "___________________________________________________________________________\n",
            "EPOCH: 96\n",
            "Total Steps: 1520\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.003021825512405485, \n",
            " Training perplexity: 1.0030263958295267\n",
            "___________________________________________________________________________\n",
            "EPOCH: 97\n",
            "Total Steps: 1536\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.0029716700955759734, \n",
            " Training perplexity: 1.0029760898841211\n",
            "___________________________________________________________________________\n",
            "EPOCH: 98\n",
            "Total Steps: 1552\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.002815191837726161, \n",
            " Training perplexity: 1.0028191582114288\n",
            "___________________________________________________________________________\n",
            "EPOCH: 99\n",
            "Total Steps: 1568\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.002792594736092724, \n",
            " Training perplexity: 1.002796497661023\n",
            "___________________________________________________________________________\n",
            "EPOCH: 100\n",
            "Total Steps: 1584\n",
            "___________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 16/16 [00:08<00:00,  1.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Good morning America | generated text:  plague. I’m make a speech, I’d leave. Call me the next day.<eos><eos>Donald Trump: (01:01:09)<eos>“Sir, you hav\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Good morning America | generated text:  life and next year will be the greatest economic year in the history of our country. That’s where w\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Very good | generated text: ratings the other night, a fake report, but the ratings go through the roof.<eos><eos>Donald Trump: (11:35)<eos>\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Very good | generated text: all over. We have to do it. It’s just we could be a little bit superstitious, right? But you people,\n",
            "\n",
            "\n",
            "Greedy decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:40:50)<eos>This group over here. They’re an activist group. Thank you very much. Thank you, fellas.<eos>\n",
            "\n",
            "Random decoding\n",
            "Sample prompt: Donald Trump: | generated text: (01:40:40)<eos>And you know what? It was a disaster. He was a disaster. He’s still a disaster actually.<eos>\n",
            "\n",
            "Training Loss: 0.0028944583027623594, \n",
            " Training perplexity: 1.0028986512916969\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "trump_samples = ['Good morning America', 'Very good', 'Donald Trump:']\n",
        "PATH = bonus_path\n",
        "trainer = Trainer(model_parameters = model_parameters, path = PATH, bptt_len = BPTT_LEN, samples = trump_samples)\n",
        "counter(PATH)\n",
        "print(\"Vocab Count: \", len(trainer.train_field.vocab))\n",
        "print(\"Count: \", len(trainer.bptt_iterator))\n",
        "trump_model = trainer.train_model(num_epochs=100)\n",
        "torch.save(trump_model.state_dict(), 'trump_model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Fable and Trump models"
      ],
      "metadata": {
        "id": "i26a5-l_aBLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fable Model\n",
        "PATH = book_path\n",
        "fable_trainer = Trainer(model_parameters = model_parameters, path = PATH, bptt_len = BPTT_LEN, samples = fable_samples)\n",
        "fable_model = fable_trainer.model\n",
        "fable_model.load_state_dict(torch.load('fable_model.pt'))\n",
        "\n",
        "# Trump Model\n",
        "PATH = bonus_path\n",
        "trump_trainer = Trainer(model_parameters = model_parameters, path = PATH, bptt_len = BPTT_LEN, samples = trump_samples)\n",
        "trump_model = trump_trainer.model\n",
        "trump_model.load_state_dict(torch.load('trump_model.pt'))\n"
      ],
      "metadata": {
        "id": "n_aqoSYxaAtA",
        "outputId": "56724671-f0b4-42e5-bd64-ed3004fae0d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxxpYCYkGFUy"
      },
      "source": [
        "# Greedy decoding for the fable model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "r4XASfKGGFUz",
        "outputId": "5d34be43-de41-467d-f90a-5f344eaf9c67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy Decoding\n",
            "A title in the book\n",
            "Sample prompt: THE FOX AND THE LION | generated text: <eos><eos>THE first time the Fox saw the Lion, he was ready to die of fear. The<eos>second time he took courage and could even bear to look upon him. The<eos>third time he had the impudence to come up to him, to salute him, and<eos>to enter into familiar conversation with him.<eos><eos><eos><eos><eos>THE DOG AND HIS SHADOW<eos><eos><eos>A DOG had sto\n",
            "A title in similar style\n",
            "Sample prompt: THE TURTLE AND THE BIRD | generated text: <eos><eos><eos>TWO Women all who had guessed the tree, the Dog indignant, and each plucking<eos>from the Jackdaw his own kind of feathers, they left the proposed king<eos>a plain Jackdaw with no claim to superior beauty.<eos><eos><eos><eos><eos>THE DONKEY AND THE LAP DOG<eos><eos><eos>THERE was once a man who had a Donkey and a little pet Dog. The Do\n",
            "Some texts in similar style\n",
            "Sample prompt: Back in my day | generated text:  and a Fox and was easily to get out of<eos>sight, and fell a prey to the dogs.<eos><eos><eos><eos><eos>THE MONKEY AND THE CAT<eos><eos><eos>A MONKEY and a Cat lived in the same family, and it was hard to tell<eos>which was the greater thief.<eos><eos>One day, as they were roaming together, they spied some chestnuts<eos>roasting in the ashes of a fir\n",
            "Anything Interesting\n",
            "Sample prompt: Dallmayr to go | generated text: when you have once<eos>seen the city you will never be willing to return to this quiet place.”<eos><eos>[Illustration]<eos><eos>After being urged a long time, the Country Mouse at last agreed to go<eos>to the city that very night. So they started off together, and about<eos>midnight came to the great house where the City Mouse\n",
            "Sample prompt: Covid-19 is | generated text: cold and comfort. People are drawn<eos>to me in their need, and they remember me afterward with gratitude. But<eos>you have nothing in you but noise, and you must be struck to make you<eos>give that out. I would not boast so much if I were you.”<eos><eos><eos><eos><eos>THE TWO FROGS<eos><eos><eos>ONCE there were two Frogs who were dear friend\n",
            "Random Decoding\n",
            "A title in the book\n",
            "Sample prompt: THE FOX AND THE LION | generated text: <eos><eos>A LITTLE fox was out playing one day, when a Lion came roaring along.<eos>“Dear me,” said the Fox, as he hid behind a tree, “I never saw a Lion<eos>before. What a terrible creature! His voice makes me tremble.”<eos><eos>The next time the Fox met the Lion he was not so much afraid, but he<eos>kept a great spring and n\n",
            "A title in similar style\n",
            "Sample prompt: THE TURTLE AND THE BIRD | generated text: <eos><eos><eos>TWO Women came the purpose of the fable is not merely to entertain but<eos>especially to point some general truth or to draw a helpful lesson,<eos>no two versions of the same came in and with him and chased the deer.<eos><eos>[Illustration]<eos><eos>At times he would snap at his prey, and at times play with him and<eos>lido\n",
            "Some texts in similar style\n",
            "Sample prompt: Back in my day | generated text:  and a Fox and was easily. Now, if you will<eos>stop quarreling and stand by each other, a should, as the fox and was burned.<eos><eos>“Oh, one you may do not see the difference between us. He was running for his life,<eos>while I was only running for my dinner.”<eos><eos><eos><eos><eos>THE OWL AND THE GRASSHOPPER<eos><eos><eos>AN Owl, who was si\n",
            "Anything Interesting\n",
            "Sample prompt: Dallmayr to go | generated text: when you have once<eos>heer no become so all that which do me all the field where well-proved by what means it<eos>meauthers and hunger, approached with great humility and begged<eos>that they would relieve his necessity with one grain of wheat or rye.<eos>One of the Ants asked him how he had disposed of his time i\n",
            "Sample prompt: Covid-19 is | generated text: creature, that you can<eos>annoy me. This ill language that you are using I regard as coming, not<eos>from you, but from the safe place on which you stand. You would be in a<eos>different mood if you were down here by my side.”<eos><eos><eos><eos><eos>THE GOURD AND THE PINE<eos><eos><eos>A GOURD was once plane in the way that it was not his, \n",
            "Sample prompt: The Angry Turtle was | generated text: on mitter place.<eos><eos>              To growing and this time in every way to a good grace from the<eos>country house, and could even bear to look upon him. The<eos>third time he had the impudence to come up to him, to salute him, and<eos>to enter into familiar conversation with him.<eos><eos><eos><eos><eos>THE DOG AND HIS SHADOW<eos><eos><eos>A D\n",
            "Greedy Decoding\n",
            "A title in the book\n",
            "Sample prompt:   | generated text:                                                                                                                                                                                                                                                                                                             \n",
            "A title in similar style\n",
            "Sample prompt: A very nice day | generated text: <eos>The leaves were so large and the flowers and fruit so fair that the<eos>Gourd, comparing them with the slender needles of the Pine, had the<eos>assurance to think itself of greater value in the comparison.<eos><eos>“Why,” said the Gourd, “you have been more years in growing to this<eos>stature than I have been days.”<eos>\n",
            "Sample prompt: I once was | generated text: able to do with them, but left the woods where they lived and<eos>came among men, building her nests in barns and along the eaves of<eos>houses.<eos><eos><eos><eos><eos>THE FARMER AND THE SNAKE<eos><eos><eos>ONE wintry day a Farmer found a Snake lying on the frozen ground, quite<eos>stiff and nearly dead with cold.<eos><eos>In a fit of compassion the\n",
            "Anything Interesting\n",
            "Sample prompt: Birds are flying | generated text: away,<eos>some laughing and others abusing it. The Cuckoo grew angry, and<eos>hastened to the Eagle with a complaint against the birds.<eos><eos>“Have pity on me!” it begged. “I have been appointed Nightingale to<eos>these woods, and yet the birds dare laugh at my singing.”<eos><eos>“My friend,” answered the Eagle, “I am a kin\n",
            "Sample prompt: Coca Cola | generated text: ing on the one<eos>here was any water in it.<eos><eos>When she looked in, she saw that there was water, but that it was so<eos>far from the top that she could not reach it, though she stretched her<eos>neck as far as she could.<eos><eos>She stopped, and thought they might be eaten. Not being able to<eos>reach them, they set to wor\n"
          ]
        }
      ],
      "source": [
        "trainer = fable_trainer\n",
        "#Greedy\n",
        "prompt = 'THE FOX AND THE LION'\n",
        "gen_text = trainer.predict(fable_model,prompt,300)\n",
        "print(\"Greedy Decoding\")\n",
        "print(\"A title in the book\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "\n",
        "#A title which you invent, which is not in the book, but similar in the style.\n",
        "prompt = 'THE TURTLE AND THE BIRD'\n",
        "gen_text = fable_trainer.predict(fable_model,prompt,300)\n",
        "print(\"A title in similar style\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "# Some texts in a similar style.\n",
        "print(\"Some texts in similar style\")\n",
        "prompt = 'Back in my day'\n",
        "gen_text = trainer.predict(fable_model,prompt,300)\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "#Anything you might find interesting\n",
        "print(\"Anything Interesting\")\n",
        "prompt = 'Dallmayr to go'\n",
        "gen_text = trainer.predict(fable_model,prompt,300)\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "prompt = 'Covid-19 is'\n",
        "gen_text = trainer.predict(fable_model,prompt,300)\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "#Multinomial\n",
        "print(\"Random Decoding\")\n",
        "print(\"A title in the book\")\n",
        "prompt = 'THE FOX AND THE LION'\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "#A title which you invent, which is not in the book, but similar in the style.\n",
        "print(\"A title in similar style\")\n",
        "prompt = 'THE TURTLE AND THE BIRD'\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "# Some texts in a similar style.\n",
        "print(\"Some texts in similar style\")\n",
        "prompt = 'Back in my day'\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "#Anything you might find interesting\n",
        "print(\"Anything Interesting\")\n",
        "prompt = 'Dallmayr to go'\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "prompt = 'Covid-19 is'\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "prompt = 'The Angry Turtle was'\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "\n",
        "#Greedy\n",
        "print(\"Greedy Decoding\")\n",
        "\n",
        "prompt = ' '\n",
        "gen_text = trainer.predict(fable_model,prompt,300)\n",
        "print(\"A title in the book\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "#A title which you invent, which is not in the book, but similar in the style.\n",
        "prompt = 'A very nice day'\n",
        "gen_text = trainer.predict(fable_model,prompt,300)\n",
        "print(\"A title in similar style\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "# Some texts in a similar style.\n",
        "prompt = 'I once was'\n",
        "gen_text = trainer.predict(fable_model,prompt,300)\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "#Anything you might find interesting\n",
        "print(\"Anything Interesting\")\n",
        "prompt = 'Birds are flying'\n",
        "gen_text = trainer.predict(fable_model,prompt,300)\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "prompt = 'Coca Cola'\n",
        "gen_text = trainer.predict(fable_model,prompt,300)\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "#Multinomial\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMM2BNGNGFUz"
      },
      "source": [
        "# Random decoding for the fable model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6kKgQ-A0GFU0",
        "outputId": "db98ba91-4e05-4a00-915b-9578cb7cf198",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Decoding\n",
            "Sample prompt:   | generated text: aid White-whisker; “she is so bold we could not frighten her. I<eos>have thought of something better than that. Let us hang a bell round<eos>her neck. Then, if she moves, the bell will ring, and we shall hear it, it<eos>he saw his mother will nature should not have been, for a year ago I was<eos>not born.”<eos><eos>“Well, \n",
            "Sample prompt: A very nice day | generated text: <eos>I meant, the Dove killed him with others, that it was<eos>not quiet like the pond, and the horses made the water muddy and foul.<eos><eos>One day the Frog from the pond said to the other, “Do come and live<eos>with me; I have plenty of food and water, and nothing to disturb me;<eos>and it is so pleasant in my pond. No\n",
            "Sample prompt: I once was | generated text: my strong rope, proyong<eos>meat, you must have a part of though I but see<eos>the way.”<eos><eos>“Oh, then we may help each other,” said the Lame Man. “If you will take<eos>me on your shoulders, we will seece if I were to place my forelegs inside the tent.”<eos><eos>“You may place your forelegs within the tent,” said the Arab\n",
            "Anything Interesting\n",
            "Sample prompt: Birds are flying | generated text: away,<eos>some laughing and others abusing it. The Cuckoo grew angry, and<eos>hastened to the Eagle with a complaint against the birds.<eos><eos>“Have pity on me!” it begged. “I have been appointed Nightingale to<eos>these woods, and yet the birds dare laugh at my singing.”<eos><eos>“My friend,” answered the Eagle, “I am a kin\n",
            "Sample prompt: Coca Cola | generated text: ing on the house.<eos><eos><eos><eos>       *       *       *       *       *       *       *       *<eos><eos>Gentle, kindly folk sometimes have to learn that kindness must be mixed<eos>with caution.<eos><eos><eos><eos><eos>THE ANTS AND THE GRASSHOPPERS<eos><eos><eos>THE Ants and the Grasshoppers lived together in the great field-.<eos><eos><eos><eos><eos>THE FOX AND THE GRAPE\n",
            "Sample prompt: The Fox | generated text: s and with the meat<eos>in his mouth, as happy as a king.<eos><eos>On the dog’s way there was a stream with a plank across it. As the<eos>water was still and clear, he stopped to take a look at it. What<eos>should he see, as he gazed into its bright depths, but a dog and was out big down to her charges, and<eos>began to pl\n",
            "Sample prompt: The Angry Turtle was | generated text: on made a<eos>breakfast, cried out, “Wolf! Wolf!” tiped at the poor Fly had nothing<eos>along the road. “But you must have some salt,” said the cook.<eos><eos>“Do you think so?” courteously replied the stranger. She gave him the<eos>sun is well up. She tried to for ne: “had you come in.”<eos><eos><eos><eos><eos>THE COCK AND THE JEWEL<eos><eos><eos>“C\n"
          ]
        }
      ],
      "source": [
        "trainer = fable_trainer\n",
        "print(\"Random Decoding\")\n",
        "prompt = ' '\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "#A title which you invent, which is not in the book, but similar in the style.\n",
        "prompt = 'A very nice day'\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "# Some texts in a similar style.\n",
        "prompt = 'I once was'\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "#Anything you might find interesting\"\n",
        "print(\"Anything Interesting\")\n",
        "prompt = 'Birds are flying'\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "prompt = 'Coca Cola'\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "prompt = 'The Fox'\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "\n",
        "prompt = 'The Angry Turtle was'\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YED1sEBHGFU1"
      },
      "source": [
        "# Greedy decoding for the trump model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OKqZ-BakGFU1",
        "outputId": "260cd825-b611-4deb-862a-d2580b89f991",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy Decoding\n",
            "Sample prompt: Thank You | generated text: for the year. “Where’s Hunter?”<eos><eos>Donald Trump: (01:57:59)<eos>You know where Hunter is? He’s buying vacuum cleaners, because he follows his father and he vacuums in money from every country that his father sees. No, seriously, Ukraine. His father was a judge of Russia, Ukraine, and China. President Obam\n",
            "Sample prompt: Good | generated text:  You know, we have to win both Nebraska’s, you know that right? You have two, you’re cut. We’re going to win both. This election is a choice between the Trump super recovery, which is what we’re having, or a Biden depression, sad to say. And some bad news just came out on Biden. But I won’t tell you\n",
            "Sample prompt: China | generated text: will own the USA. If Biden wins he will do bad things. By the way, he’s not a good guy. Just so you understand.<eos><eos>Donald Trump: (08:20)<eos>Is there any place you would rather be than a Trump rally on about a 10 degree evening? 10 degrees. Yeah, it’s cold out here, but that’s okay. You know, we just left\n",
            "Sample prompt: We have to | generated text: a find on election. The only bad part is I probably wouldn’t be standing out here in the freezing rain with you. I’d be home in the White House, doing whatever the hell I was doing. I wouldn’t be out here. And we’re going to do other great places after this. You know that, right? This is stop number\n"
          ]
        }
      ],
      "source": [
        "trainer = trump_trainer\n",
        "#Greedy\n",
        "prompt = 'Thank You'\n",
        "gen_text = trainer.predict(trump_model,prompt,300)\n",
        "print(\"Greedy Decoding\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "#A title which you invent, which is not in the book, but similar in the style.\n",
        "prompt = 'Good'\n",
        "gen_text = trainer.predict(trump_model,prompt,300)\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "# Some texts in a similar style.\n",
        "prompt = 'China'\n",
        "gen_text = trainer.predict(trump_model,prompt,300)\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "#Anything you might find interesting\n",
        "prompt = 'We have to'\n",
        "gen_text = trainer.predict(trump_model,prompt,300)\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbVBsPwsGFU1"
      },
      "source": [
        "# Random decoding for the trump model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "axOZ54UGGFU1",
        "outputId": "0622a2bd-5646-4248-e0fa-5aecf833dab1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Decoding\n",
            "Sample prompt: Thank You | generated text: for the great people of… How many times did he do that? Seven? Standing up for the great people of Iowa. “Sir, sir, it’s Michigan.” There’s no recovery from that, right? When you do that, you just walk off the stage, right? I always say the great Winston Churchill, he was a great orator, even if Win\n",
            "Sample prompt: Good | generated text:  Thank you. Thank you. Thank you must more things to do. I try and stay uninvolved. Going too slowly. “No sir, we’d rather not do anything before the election.” Well they did it to me before, they did during, before, and after. These are bad people, Comey, the worst director in the history of the FB\n",
            "Sample prompt: China | generated text: plague. That’s what I tested positive for. But it was amazing. And it was actually an experience.<eos><eos>Donald Trump: (26:04)<eos>And until I came along, you were supposed to have lifetime immunity. Right? I said, “Well, I’m recovered. I feel great. And I’m immune.” I could jump right into this road. I could\n",
            "Sample prompt: We have to | generated text: finish off here. We have to do it. It’s just we could be a little bit superstitious, right? But you peopl, you will see you that right now. I’m telling you that right now, that if people are, I hate to see where they put boards up on a window, this shouldn’t be that, it shouldn’t be that, but we are\n",
            "Sample prompt:   | generated text: he right to life, and the right to keep and bear arms.<eos><eos>Donald Trump: (43:27)<eos>We will maintain America’s unrivaled military might, and we will ensure peace through strength. We will end surprise medical billing, require price transparency, already signed January 1st, lower drug prices, even more, and we will always protect patients with pre- existing conditions. We will stop the radical indoctrination of our students and restore patriotic education to our schools. We will teach our children to love our country, honor our history and always respect our great American flag. And we will live by the timeless words of our national motto, in God we trust. For years you had a president who apologized for America. Now you have a president who is standing up for America and standing up for the great people of Michigan. See, now Biden would do that, and he’d say for the great people of… How many times did he do that? Seven? Standing up for the great people of Iowa. “Sir, sir, it’s Michigan.” The\n",
            "Sample prompt:   | generated text: 22:49)<eos>Doesn’t he deserve some credit for that? It’s better. The USMCA is better than NAFTA.<eos><eos>Joe Biden: (22:53)<eos>It is better than NAFTA.<eos><eos>Joe Biden: (22:57)<eos>I have never said I oppose fracking.<eos><eos>Donald Trump: (23:01)<eos>You said it on tape.<eos><eos>Joe Biden: (23:02)<eos>I did … Show the tape, put it on your website.<eos><eos>Donald Trump: (23:05)<eos>I’ll put it on.<eos><eos>Joe Biden: (23:05)<eos>Put it on the website.<eos><eos>Dana Bash: (23:08)<eos>Would there be any place for fossil fuels, including coal and fracking in a Biden administration?<eos><eos>Joe Biden: (23:15)<eos>No, we would work it out. We would make sure it’s eliminated. I guaranteed we’re going to end fossil fuel. No more, no new fracking. I’d gradually move away from fracking.<eos><eos>Kamala Harris: (23:28)<eos>And I think it’s critically important on day one that we end any fossil fuel leases on public lands.<eos><eos>Speaker 2: (23:35)<eos>What about states stopping fracking and stopping pipeline infrastructure?<eos><eos>Joe Biden: (23:39)<eos>Yeah. New pipeline, exactly.<eos><eos>Kamala Harris: (23:41)<eos>There’s no\n",
            "Something not in the text\n"
          ]
        }
      ],
      "source": [
        "trainer = trump_trainer\n",
        "\n",
        "print(\"Random Decoding\")\n",
        "prompt = 'Thank You'\n",
        "gen_text = trainer.predict(trump_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "#A title which you invent, which is not in the book, but similar in the style.\n",
        "prompt = 'Good'\n",
        "gen_text = trainer.predict(trump_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "# Some texts in a similar style.\n",
        "prompt = 'China'\n",
        "gen_text = trainer.predict(trump_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "#Anything you might find interesting\n",
        "prompt = 'We have to'\n",
        "gen_text = trainer.predict(trump_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "prompt = ' '\n",
        "gen_text = trainer.predict(trump_model,prompt,1000,method=\"greedy\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "prompt = ' '\n",
        "gen_text = trainer.predict(trump_model,prompt,1000,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "print(\"Something not in the text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhB1N00bGFU2"
      },
      "source": [
        "# Comparing random and greedy for both models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wijMZenWGFU2",
        "outputId": "b7d03a2a-e93c-4698-fd44-89e6a2bb13af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Decoding\n",
            "Sample prompt: Birds fly high | generated text: up a forest, and made<eos>redy some one to advise.<eos><eos><eos><eos><eos>THE KID AND THE WOLF<eos><eos><eos>A KID coming home alone one night met a big Wolf. “Oh, oh, I know you<eos>will kill me,” said the little Kid; “but please play me a tune, so that<eos>I may have one more dance before I die; I am so fond of dancing.”<eos><eos>“Very well,” said\n",
            "Greedy Decoding\n",
            "Sample prompt: Birds fly high | generated text: up a forest, and made<eos>ready to spring upon him. Away went the stag! and the legs that he had<eos>despised would soon have carried him out of danger; but when he came to<eos>the thick woods, his beautiful antlers, of which he had been so vain,<eos>caught in the branches and held him fast until the lion came up a\n",
            "Sample prompt: <eos> | generated text: and it. She stones that he might<eos>persuade to help her pick up the seeds which the farmer had sown.<eos><eos>By and by the flax sprang up, and the Swallow tried again to persuade<eos>the Birds to pull the young flax before it grew large. But they all<eos>made fun of her caution and let the flax keep growing.<eos><eos>When she saw how heedless all the Birds were, the little field<eos>Mouse, which from grain of the man’s misfortune, he dived to the bottom<eos>of the river, and bringing up a gold’s legs and was kindly<eos>taken that he bade her ask what she would in exchange.<eos><eos>The Bee, who was nursing a private spite for the loss of\n",
            "Sample prompt: <eos> | generated text: and in back and fould.<eos><eos>Persure in put the ill not a bird, and to prove it, called attention to his mouselike head and<eos>ears, which so confused the Weasel that he let the Bat go.<eos><eos>Some time afterward, on another flight, the Bat fell again to the<eos>ground, and another Weasel caught him. On perceiving that the Weasel<eos>thought he was a mouse, the Bat contended that he had wings and<eos>therefore was not a mouse, and was again allowed to go free.<eos><eos><eos><eos><eos>THE FOX AND THE GOAT<eos><eos><eos>A FOX who had fallen into a deep well was casting about to find how he<eos>should get out again. At length a Goat came to the place, and s\n",
            "Sample prompt: THE WOLF AND THE LAMB | generated text: <eos><eos>A WOLF and a Lamb came to a running brook to quench their thirst. The<eos>Wolf stood high up the stream and the Lamb a little distance below.<eos>Having made up his mind to seize the Lamb, the Stork urged<eos>that he was neither goose nor crane but a poor, harmless Stork, who<eos>performed his duty to his parents as well as ever he could think of<eos>that was pleasant to see; and the Frog invited the Mouse to go home<eos>with him and see all the beautiful things that are under the water.<eos><eos>“Can you swim?” asked the Frog.<eos><eos>“Not much,” said the Mouse.<eos><eos>[Illustration]<eos><eos>“No matter,” said the Frog; “I will tie your foot to my foot with a<eos>piece of this strong grass, and then I can pull you along nicely.” The<eos>Frog laughed as he said this. He thought it would be good fun for him,<eos>but he well knew that the Mouse would not enjoy it.<eos><eos>When the Frog had tied the Mouse’s foot to his own, they started<eos>together across the meadow. They soon came to the edge of the water,<eos>and the Frog jumped in, pulling the Mouse in with him\n",
            "Sample prompt: THE WOLF AND THE LAMB | generated text: <eos><eos>A WOLF and a Lamb came to a running brook to quench their thirst. The<eos>Wolf stood high up the stream and the Lamb a little distance below.<eos>Having made up his mind to seize the Lamb, the Wolf bethought himself<eos>how he might justify his act of violence. Running down to her, he<eos>roared, “How dare you muddle the water so that I cannot drink it?”<eos><eos>The Lamb, affrighted by the charge, humbly replied that she could not<eos>see how that could be, since the water ran down from him to her and not<eos>from her to him. “Be that as it may,” retorted the Wolf. “You are a<eos>rascal, all the same, and I have heard that you said bad things of me<eos>last year behind my back.”<eos><eos>“Nay,” said the Lamb, “that could not have been, for a year ago I was<eos>not born.”<eos><eos>“Well, if it was not you it was your father, and that is all the same,”<eos>replied the Wolf, and he fell upon the Lamb and tore her to pieces.<eos><eos><eos><eos><eos>THE FOX AND THE LION<eos><eos><eos>THE first time the Fox saw the Lion, he was ready to die of fear. The<eos>second time he took courage a\n",
            "Greedy Decoding\n",
            "Sample prompt: President Donald J. Trump:  | generated text: 01:14)<eos>Thank you very much and hello, Kenosha. It’s nice to be back. It’s nice to be back. We spent a little time with you, a little law and order. We brought law and order to Kenosha. Right? That’s what we want. And hello, Wisconsin. Big day, tomorrow, big, big day, big day. And I think we’re going to do very well in Wisconsin just like we did four years ago. And it’s an honor to be with you. Thank you.<eos><eos>Audience: (01:41)<eos>USA! USA! USA! USA!<eos><eos>President Donald J. Trump: (01:51)<eos>And this is a lot of people. This is a lot of people. See, you know what that means? That means we don’t have to pay \n",
            "Random Decoding\n",
            "Sample prompt: President Donald J. Trump:  | generated text: 01:14)<eos>Thank you very much and hello, Kenosha. It’s nice to be back. It’s nice to be back. We spent a little time with you, a little law and order. We brought law and order to Kenosha. Right? That’s what we want. And hello, Wisconsin. Big day, tomorrow, big, big day, big day. And I think we’re going to do very well in Wisconsin just like we did four years ago. And it’s an honor to be with you. Thank you.<eos><eos>Audience: (01:41)<eos>USA! USA! USA! USA!<eos><eos>President Donald J. Trump: (01:51)<eos>And this like to say this, up the state, right? and I mean, we’re saving the suburbs. This regulation was so horrible\n"
          ]
        }
      ],
      "source": [
        "trainer = fable_trainer\n",
        "print(\"Random Decoding\")\n",
        "prompt = 'Birds fly high'\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "prompt = 'Birds fly high'\n",
        "print(\"Greedy Decoding\")\n",
        "gen_text = trainer.predict(fable_model,prompt,300,method=\"greedy\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "prompt = '<eos>'\n",
        "gen_text = trainer.predict(fable_model,prompt,600,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "gen_text = trainer.predict(fable_model,prompt,600,method=\"greedy\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "\n",
        "prompt = 'THE WOLF AND THE LAMB'\n",
        "gen_text = trainer.predict(fable_model,prompt,1000,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "prompt = 'THE WOLF AND THE LAMB'\n",
        "gen_text = trainer.predict(fable_model,prompt,1000,method=\"greedy\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "\n",
        "trainer = trump_trainer\n",
        "print(\"Greedy Decoding\")\n",
        "prompt = \"President Donald J. Trump: \"\n",
        "gen_text = trainer.predict(trump_model,prompt,600,method=\"greedy\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
        "print(\"Random Decoding\")\n",
        "prompt = \"President Donald J. Trump: \"\n",
        "gen_text = trainer.predict(trump_model,prompt,600,method=\"random\")\n",
        "print(f'Sample prompt: {prompt} | generated text: {gen_text}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RsitEVA3mq20"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.3 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}