{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "from torchtext.legacy.data import Field\n",
    "from torchtext.legacy.datasets import LanguageModelingDataset\n",
    "from torchtext.legacy.data import BPTTIterator\n",
    "from src.model import LSTMModel\n",
    "from src.helper import counter, get_fables\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, model_parameters : dict, path : str, bptt_len : int, samples : list):\n",
    "        self.samples = samples\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        _split_chars = lambda x: list(x) \n",
    "        self.train_field = Field(tokenize=_split_chars ,init_token ='<sos>',eos_token ='<eos>')\n",
    "        train_dataset = LanguageModelingDataset(\n",
    "            path = path,\n",
    "            text_field=self.train_field\n",
    "        )\n",
    "        self.train_field.build_vocab(train_dataset)\n",
    "        self.bptt_iterator = BPTTIterator(\n",
    "            dataset= train_dataset,\n",
    "            batch_size = model_parameters['batch_size'],\n",
    "            bptt_len = bptt_len,\n",
    "            shuffle = False\n",
    "        )\n",
    "        self.model = LSTMModel(**model_parameters, vocab_size = len(self.train_field.vocab)).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        \n",
    "    def predict(self, model : LSTMModel, prompt : str,sequence_length : int, method : str = 'greedy') -> str:\n",
    "        \"\"\"\n",
    "        The function takes in a model, a prompt, a sequence length, and a method. It then generates a\n",
    "        sentence of the specified length using the specified method\n",
    "        \n",
    "        :param model: the model to use for prediction\n",
    "        :type model: LSTMModel\n",
    "        :param prompt: The prompt to start the sentence with\n",
    "        :type prompt: str\n",
    "        :param sequence_length: The length of the generated sequence\n",
    "        :type sequence_length: int\n",
    "        :param method: 'greedy' or 'random', defaults to greedy\n",
    "        :type method: str (optional)\n",
    "        :return: A string of the generated sentence\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        generated_sentence=[]\n",
    "        prompt = torch.tensor([self.train_field.vocab.stoi[t] for t in self.train_field.tokenize(prompt)]).long().to(self.device)\n",
    "        hidden = None\n",
    "        Softmax1D = nn.Softmax(dim=1)\n",
    "        if method == 'greedy':\n",
    "            out,hidden=model(prompt.view(-1,1),hidden)\n",
    "            print(hidden[0].shape)\n",
    "            ix = torch.argmax(Softmax1D(out), dim=1)[len(prompt)-1]\n",
    "            for i in range(sequence_length):\n",
    "                out,hidden=model(ix.view(-1,1),hidden)\n",
    "                ix = torch.argmax(Softmax1D(out), dim=1)\n",
    "                generated_sentence.append(self.train_field.vocab.itos[ix])\n",
    "        if method == 'random':\n",
    "            out,hidden=model(prompt.view(-1,1),hidden)\n",
    "            ix = torch.multinomial(Softmax1D(out),1)[len(prompt)-1]\n",
    "            for i in range(sequence_length):\n",
    "                out,hidden=model(ix.view(-1,1),hidden)\n",
    "                ix = torch.multinomial(Softmax1D(out),1)\n",
    "                generated_sentence.append(self.train_field.vocab.itos[ix])\n",
    "        return ''.join(generated_sentence)\n",
    "\n",
    "    def train_model(self, num_epochs : int) -> LSTMModel:\n",
    "        \"\"\"\n",
    "        The function takes in the number of epochs and the model and trains the model for the given\n",
    "        number of epochs\n",
    "        \n",
    "        :param num_epochs: Number of epochs to train for\n",
    "        :return: The model is being returned.\n",
    "        \"\"\"\n",
    "        # vocab_size = \n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "        total_steps = 0\n",
    "        loss_plot =[]\n",
    "        perp_plot=[]\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            cost = 0\n",
    "            num_steps =0\n",
    "            hidden=None\n",
    "            print('Total Steps: ',total_steps)\n",
    "            for batch in self.bptt_iterator:\n",
    "                self.model.train()\n",
    "                self.optimizer.zero_grad()\n",
    "                output, hidden = self.model(batch.text.to(self.device),hidden)\n",
    "                hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "                targets = batch.target\n",
    "                targets = targets.view(targets.shape[0]*targets.shape[1]).to(device)\n",
    "                out = output.view(-1, self.model.vocab_size)\n",
    "                loss = loss_fn(out,targets)\n",
    "                cost += loss.item()\n",
    "                num_steps += 1\n",
    "                total_steps+=1\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            if epoch%1==0:\n",
    "                self.model.eval()\n",
    "                for prompt in self.samples:\n",
    "                    print('Greedy decoding')\n",
    "                    gen_text = self.predict(self.model, prompt, 100)\n",
    "                    print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "                    print('Random decoding')\n",
    "                    gen_text = self.predict(self.model, prompt, 100, method='random')\n",
    "                    print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "            perplexity = np.exp(cost/num_steps)\n",
    "            loss_plot.append(cost/num_steps)\n",
    "            perp_plot.append(perplexity)\n",
    "            print('epoch:',epoch,'loss: ', cost/num_steps, 'perplexity:',perplexity)\n",
    "        _, ax1 = plt.subplots()\n",
    "        _, ax2 = plt.subplots()\n",
    "        ax1.plot(loss_plot,'coral')\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.title.set_text('Train Loss Plot')\n",
    "        ax1.legend([\"Train Loss\"])\n",
    "        ax2.plot(perp_plot,'deepskyblue')\n",
    "        ax2.set_xlabel('Epoch')\n",
    "        ax2.set_ylabel('Loss')\n",
    "        ax2.title.set_text('Train Perplexity Plot')\n",
    "        ax2.legend([\"Train Perplexity\"])\n",
    "        plt.show()\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the fable and the trump models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fable_samples = ['Dogs like best to', 'THERE were once some Frogs who lived together', 'THE WOMAN AND HER HEN']\n",
    "get_fables()\n",
    "book_path = os.path.join(os.path.join(\"data\",\"books\"), \"AesopsFables.txt\")\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 1024\n",
    "HIDDEN_SIZE = 1024\n",
    "LAYERS = 2\n",
    "PATH = book_path\n",
    "BPTT_LEN = 256\n",
    "model_parameters = {\n",
    "    'batch_size' : BATCH_SIZE,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': LAYERS\n",
    "}\n",
    "trainer = Trainer(model_parameters = model_parameters, path = PATH, bptt_len = BPTT_LEN, samples = fable_samples)\n",
    "counter(PATH)\n",
    "print(\"Vocab Count: \", len(trainer.train_field.vocab))\n",
    "print(\"Count: \", len(trainer.bptt_iterator))\n",
    "fable_model = trainer.train_model(epoch_num=100)\n",
    "torch.save(fable_model.state_dict(), 'fable_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_samples = ['Good morning America', 'Very good', 'Donald Trump:']\n",
    "bonus_path = os.path.join('data', \"donaldtrump.txt\")\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 1024\n",
    "HIDDEN_SIZE = 1024\n",
    "LAYERS = 2\n",
    "PATH = bonus_path\n",
    "BPTT_LEN = 256\n",
    "model_parameters = {\n",
    "    'batch_size' : BATCH_SIZE,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'hidden_size': HIDDEN_SIZE,\n",
    "    'num_layers': LAYERS\n",
    "}\n",
    "trainer = Trainer(model_parameters = model_parameters, path = PATH, bptt_len = BPTT_LEN, samples = trump_samples)\n",
    "counter(PATH)\n",
    "print(\"Vocab Count: \", len(trainer.train_field.vocab))\n",
    "print(\"Count: \", len(trainer.bptt_iterator))\n",
    "trump_model = trainer.train_model(epoch_num=100)\n",
    "torch.save(trump_model.state_dict(), 'trump_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy decoding for the fable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Greedy\n",
    "\n",
    "prompt = 'THE FOX AND THE LION'\n",
    "gen_text = trainer.predict(model,prompt,300)\n",
    "print(\"Greedy Decoding\")\n",
    "print(\"A title in the book\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "\n",
    "#A title which you invent, which is not in the book, but similar in the style.\n",
    "prompt = 'THE TURTLE AND THE BIRD'\n",
    "gen_text = trainer.predict(model,prompt,300)\n",
    "print(\"A title in similar style\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "# Some texts in a similar style.\n",
    "print(\"Some texts in similar style\")\n",
    "prompt = 'Back in my day'\n",
    "gen_text = trainer.predict(model,prompt,300)\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "#Anything you might find interesting\n",
    "print(\"Anything Interesting\")\n",
    "prompt = 'Dallmayr to go'\n",
    "gen_text = trainer.predict(model,prompt,300)\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "prompt = 'Covid-19 is'\n",
    "gen_text = trainer.predict(model,prompt,300)\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "#Multinomial\n",
    "print(\"Random Decoding\")\n",
    "print(\"A title in the book\")\n",
    "prompt = 'THE FOX AND THE LION'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "#A title which you invent, which is not in the book, but similar in the style.\n",
    "print(\"A title in similar style\")\n",
    "prompt = 'THE TURTLE AND THE BIRD'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "# Some texts in a similar style.\n",
    "print(\"Some texts in similar style\")\n",
    "prompt = 'Back in my day'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "#Anything you might find interesting\n",
    "print(\"Anything Interesting\")\n",
    "prompt = 'Dallmayr to go'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "prompt = 'Covid-19 is'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "# prompt = 'The Angry Turtle was'\n",
    "# gen_text = predict(model,prompt,300,method=\"random\")\n",
    "# print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "\n",
    "#Greedy\n",
    "print(\"Greedy Decoding\")\n",
    "\n",
    "prompt = ' '\n",
    "gen_text = trainer.predict(model,prompt,300)\n",
    "print(\"A title in the book\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "#A title which you invent, which is not in the book, but similar in the style.\n",
    "prompt = 'A very nice day'\n",
    "gen_text = trainer.predict(model,prompt,300)\n",
    "print(\"A title in similar style\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "# Some texts in a similar style.\n",
    "prompt = 'I once was'\n",
    "gen_text = trainer.predict(model,prompt,300)\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "#Anything you might find interesting\n",
    "print(\"Anything Interesting\")\n",
    "prompt = 'Birds are flying'\n",
    "gen_text = trainer.predict(model,prompt,300)\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "prompt = 'Coca Cola'\n",
    "gen_text = trainer.predict(model,prompt,300)\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "#Multinomial\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random decoding for the fable model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Decoding\")\n",
    "prompt = ' '\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "#A title which you invent, which is not in the book, but similar in the style.\n",
    "prompt = 'A very nice day'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "# Some texts in a similar style.\n",
    "prompt = 'I once was'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "#Anything you might find interesting\n",
    "print(\"Anything Interesting\")\n",
    "prompt = 'Birds are flying'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "prompt = 'Coca Cola'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "prompt = 'The Fox'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "\n",
    "prompt = 'The Angry Turtle was'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "\n",
    "prompt = '<eos>'\n",
    "gen_text = trainer.predict(model,prompt,600,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "gen_text = trainer.predict(model,prompt,600,method=\"greedy\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "\n",
    "prompt = 'THE WOLF AND THE LAMB'\n",
    "gen_text = trainer.predict(model,prompt,1000,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "prompt = 'THE WOLF AND THE LAMB'\n",
    "gen_text = trainer.predict(model,prompt,1000,method=\"greedy\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy decoding for the trump model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Greedy\n",
    "prompt = 'Thank You'\n",
    "gen_text = trainer.predict(model,prompt,300)\n",
    "print(\"Greedy Decoding\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "#A title which you invent, which is not in the book, but similar in the style.\n",
    "prompt = 'Good'\n",
    "gen_text = trainer.predict(model,prompt,300)\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "# Some texts in a similar style.\n",
    "prompt = 'China'\n",
    "gen_text = trainer.predict(model,prompt,300)\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "#Anything you might find interesting\n",
    "prompt = 'We have to'\n",
    "gen_text = trainer.predict(model,prompt,300)\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random decoding for the trump model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Decoding\")\n",
    "prompt = 'Thank You'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "#A title which you invent, which is not in the book, but similar in the style.\n",
    "prompt = 'Good'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "# Some texts in a similar style.\n",
    "prompt = 'China'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "#Anything you might find interesting\n",
    "prompt = 'We have to'\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "prompt = ' '\n",
    "gen_text = trainer.predict(model,prompt,1000,method=\"greedy\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "prompt = ' '\n",
    "gen_text = trainer.predict(model,prompt,1000,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "print(\"Something not in the text\")\n",
    "prompt = 'Birds fly high'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing random and greedy for the trump model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Random Decoding\")\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "prompt = 'Birds fly high'\n",
    "print(\"Greedy Decoding\")\n",
    "gen_text = trainer.predict(model,prompt,300,method=\"greedy\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "\n",
    "print(\"Greedy Decoding\")\n",
    "prompt = \"President Donald J. Trump: \"\n",
    "gen_text = trainer.predict(model,prompt,600,method=\"greedy\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')\n",
    "print(\"Random Decoding\")\n",
    "prompt = \"President Donald J. Trump: \"\n",
    "gen_text = trainer.predict(model,prompt,600,method=\"random\")\n",
    "print(f'Sample prompt: {prompt} | generated text: {gen_text}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
